>(ì½”ë“œì‹¤ìŠµ-ëª…ì„¸íë¦„) ì‹¤ì œ í”Œì  ë³´ë©´ì„œ íë¦„ íŒŒì•… ë° ì‚¬ìš©ëœ ëª¨ë¸ í•™ìŠµ
>
>ì´ë¯¸ì§€ ìº¡ì…”ë‹ í™œìš©í•œ í”Œì  ë³´ë©´ì„œ ì½”ë“œ í•˜ë‚˜ì”© ê³µë¶€í•˜ê³  ì •ë¦¬ ì¤‘
>
>ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì‹¤ì œë¡œ ì–´ë–»ê²Œ ì“°ì´ëŠ”ì§€ 

âœ í•™ìŠµë‚´ìš© (ì½”ë“œ)



 ### 1. í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬

#### (1) ë¨¸ì‹ ëŸ¬ë‹

- `Numpy` : í–‰ë ¬ì´ë‚˜ ëŒ€ê·œëª¨ ë‹¤ì°¨ì› ë°°ì—´ì„ ì‰½ê²Œ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ ì§€ì›í•˜ëŠ” íŒŒì´ì¬ ë¼ì´ë¸ŒëŸ¬ë¦¬

  ì‚¬ìš©ì˜ˆ) ë°°ì—´ ìƒì„±, ë‚´ì¥í•¨ìˆ˜ ì´ìš©í•˜ì—¬ ê°ì¢… ìˆ˜ì¹˜ ê³„ì‚°ì„ ìœ„í•œ ê¸°ë³¸ íŒ¨í‚¤ì§€

- `Scipy` : ê³¼í•™ ì»´í“¨íŒ…ê³¼ ê¸°ìˆ  ì»´í“¨íŒ…ì— ì‚¬ìš©ë˜ëŠ” ìˆ˜í•™, ê³¼í•™, ê³µí•™ì— ëŒ€í•œ ììœ -ì˜¤í”ˆ ì†ŒìŠ¤ íŒŒì´ì¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ => ì‹ í˜¸ì²˜ë¦¬, ìµœì í™”, í†µê³„ ë“±ì˜ ê¸°ëŠ¥ì„ ê°–ëŠ” íŒ¨í‚¤ì§€ë¥¼ ë¬¶ì–´ë†“ìŒ

- `Scikit-learn` : ê¸°ê³„ í•™ìŠµ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ë¶„ë¥˜, íšŒê·€ ë¶„ì„ ë° í´ëŸ¬ìŠ¤í„°ë§ í¬í—˜ ì•Œê³ ë¦¬ì§ SVM, ëœë¤í¬ë ˆìŠ¤íŠ¸, ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŠ¸ ë“±, numpy, scipy ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ ìƒí˜¸ì‘ìš©í•˜ë„ë¡ ì„¤ê³„ë¨

#### (2) ë”¥ëŸ¬ë‹

- `Tensorflow` : ë”¥ëŸ¬ë‹ê³¼ ê¸°ê³„í•™ìŠµ ë¶„ì•¼ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬

  ì‚¬ìš©ì˜ˆ) ë°ì´í„°í”Œë¡œìš°ê·¸ë˜í”„ë¥¼ í†µí•œ í’ë¶€í•œ í‘œí˜„ë ¥, ê³„ì‚° êµ¬ì¡°ì™€ ëª©í‘œ í•¨ìˆ˜ë§Œ ì •ì˜í•˜ë©´ ìë™ìœ¼ë¡œ ë¯¸ë¶„ ê³„ì‚° ì²˜ë¦¬ 

- `Keras` : ì˜¤í”ˆì†ŒìŠ¤ ì‹ ê²½ë§ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ë”¥ ì‹ ê²½ë§ê³¼ì˜ ë¹ ë¥¸ ì‹¤í—˜ì„ ê°€ëŠ¥ì¼€ í•˜ë„ë¡ ì„¤ê³„ë¨

#### (3) ì‹œê°í™”

- `Matplotlib` : íŒŒì´ì¬ ê³¼í•™ ìƒíƒœê³„ì˜ í‘œì¤€ ê·¸ë˜í”„ íŒ¨í‚¤ì§€, ëŒ€ë¶€ë¶„ì˜ ê·¸ë˜í”„ë¥¼ ê·¸ë¦´ ìˆ˜ ìˆìŒ

  ì‚¬ìš©ì˜ˆ) ì„ ê·¸ë˜í”„, ì‚°ì ë„, ë„˜íŒŒì´ ë°°ì—´ë¡œ ì‚°ì ë„ ê·¸ë¦¬ê¸°

- `Tensorboard` : í•™ìŠµê³¼ì • ëª¨ë‹ˆí„°ë§ íˆ´. í…ìŠ¤í”Œë¡œìš° ê¸°ë°˜ìœ¼ë¡œ ì¼€ë¼ìŠ¤ë¥¼ êµ¬ë™í•  ê²½ìš° ì‚¬ìš© ê°€ëŠ¥í•¨

#### (4) ê¸°íƒ€

- `tqdm`  : í•¨ìˆ˜ë‚˜ ë°˜ë³µë¬¸ì˜ ì§„í–‰ìƒí™©ì„ progressbarë¡œ ë‚˜íƒ€ë‚´ì£¼ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬
- `scikit-image` : ì´ë¯¸ì§€ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ (load, save ë“±)





> **<u>ì°¸ê³ í•˜ë©´ ì¢‹ì„ ìë£Œ</u>**
>
> - ì´ë¯¸ì§€ ìº¡ì…”ë‹ ì „ì²´ íë¦„ https://www.tensorflow.org/tutorials/text/image_captioning
>
> - í…ì„œë³´ë“œ ì‚¬ìš© ë°©ë²• https://blog.naver.com/keeping816/221670552687
>
> - inception ëª¨ë¸ì— ê´€í•œ ì„¤ëª… https://norman3.github.io/papers/docs/google_inception.html
>
> - inception v3 ê³ ê¸‰ ê°€ì´ë“œ https://cloud.google.com/tpu/docs/inception-v3-advanced?hl=ko
>
> - ë„˜íŒŒì´ì˜ ê¸°ì´ˆí†µê³„í•¨ìˆ˜ (í‰ê· , ë¶„ì‚°, í‘œì¤€í¸ì°¨, ê³µë¶„ì‚°) https://kongdols-room.tistory.com/70
>
> - íŒŒì´ì¬ pickle ëª¨ë“ˆ https://wayhome25.github.io/cs/2017/04/04/cs-04/
>
> - TensorFlowì—ì„œ Datasetì„ ì‚¬ìš©í•˜ëŠ” ë°©ë²• https://cyc1am3n.github.io/2018/09/13/how-to-use-dataset-in-tensorflow.html
>
> - TensorFlow Dataset API https://datascienceschool.net/view-notebook/57714103a75c43ed9a7d95f96135f0ad/
>
> - Augmentation https://nittaku.tistory.com/272 
>
>   â€‹						 https://deepestdocs.readthedocs.io/en/latest/003_image_processing/0030/
>
>   - tf.data APIë¡œ ì„±ëŠ¥ í–¥ìƒí•˜ê¸° https://www.tensorflow.org/guide/data_performance?hl=ko
>
> - ëª¨ë“ˆ tf.keras.preprocessing.image https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image
>
> - ëª¨ë“ˆ tf.image https://www.tensorflow.org/api_docs/python/tf/image?hl=ko-KRChoosing&skip_cache=true
>
> - ì‹ ê²½ë§ ì´í•´í•˜ê¸° (tf.keras.layers.Dense) https://mansculture.com/%EC%8B%A0%EA%B2%BD%EB%A7%9D-%EC%9D%B4%ED%95%B4%ED%95%98%EA%B8%B0-tf-keras-layers-dens/



### 2. í”„ë¡œì íŠ¸ íë¦„

#### (1) Data Preprocessing

- Load dataset > Load image data > `CNN` > Extract features of image > Tokenize captions

#### (2) Train & Test Model

- Encode features > Decode features > Calculate loss > Backpropagation > test model



## :deciduous_tree: Directory Architecture

```
â”œâ”€â”€ README.md                 - README.md
â”œâ”€â”€ MD_imgs/                  - README.mdì— ì‚½ì…ëœ ì´ë¯¸ì§€ê°€ ì €ì¥ëœ í´ë”
â”œâ”€â”€ Daily/                    - Daily íŒ€íšŒì˜ ë‚´ìš©ê³¼ ê° íŒ€ì›ì˜ Daily commit mdê°€ ì €ì¥ëœ í´ë”
â”‚
|
â”œâ”€â”€ args/                     - í•™ìŠµ ì‹œ ì‚¬ìš©ëœ config ì •ë³´ê°€ ì €ì¥ë˜ëŠ” í´ë”
â”œâ”€â”€ checkpoints/              - í•™ìŠµëœ ëª¨ë¸ì´ ì €ì¥ë˜ëŠ” í´ë”
â”‚   â”œâ”€â”€ CNN/                 - Featureë¥¼ ì¶”ì¶œí•˜ëŠ” CNN ëª¨ë¸ì„ ì €ì¥í•˜ëŠ” í´ë”
â”‚   â”œâ”€â”€ ckpt/                 - ì²´í¬í¬ì¸íŠ¸ê°€ ì €ì¥ë˜ëŠ” í´ë”
|
â”œâ”€â”€ data/                     - ë°ì´í„° ì „ì²˜ë¦¬ ê´€ë ¨ í´ë”
â”‚   â”œâ”€â”€ __init__.py           - 
â”‚   â”œâ”€â”€ preprocess.py         - ë°ì´í„° ì „ì²˜ë¦¬
|
â”œâ”€â”€ datasets/                 - ì „ì²´ ë°ì´í„°ì…‹
â”‚   â”œâ”€â”€ images/               - ì „ì²´ ì´ë¯¸ì§€ê°€ ë“¤ì–´ìˆëŠ” í´ë”
â”‚   â”œâ”€â”€ textTokenizers/       - í† í°í™”ëœ ìº¡ì…˜ì´ ì €ì¥ë˜ëŠ” í´ë”(.txt)
â”‚   â”œâ”€â”€ captions.csv          - images í´ë”ì˜ ì´ë¯¸ì§€ íŒŒì¼ëª… + ìº¡ì…˜ ëª©ë¡
â”‚   â”œâ”€â”€ linear_test_x.npy     - ìŠ¤ì¼ˆë ˆí†¤ ì½”ë“œ ì‹¤í–‰ì„ ìœ„í•œ ìƒ˜í”Œ ë°ì´í„°(test)
â”‚   â”œâ”€â”€ linear_train.npy      - ìŠ¤ì¼ˆë ˆí†¤ ì½”ë“œ ì‹¤í–‰ì„ ìœ„í•œ ìƒ˜í”Œ ë°ì´í„°(train)
â”‚   â”œâ”€â”€ train.txt             - train ë°ì´í„°ëª©ë¡(ì´ë¯¸ì§€ íŒŒì¼ëª…+ìº¡ì…˜)
â”‚   â”œâ”€â”€ val.txt               - test ë°ì´í„°ëª©ë¡(ì´ë¯¸ì§€ íŒŒì¼ëª…+ìº¡ì…˜)
|
â”œâ”€â”€ models/                   - 
â”‚   â”œâ”€â”€ __init__.py           - 
â”‚   â”œâ”€â”€ decoder.py            - í…ìŠ¤íŠ¸ ëª¨ë¸ë§
â”‚   â”œâ”€â”€ encoder.py            - ì´ë¯¸ì§€ ëª¨ë¸ë§
â”‚   â”œâ”€â”€ linear_model.py       - ìŠ¤ì¼ˆë ˆí†¤ ì½”ë“œì— ì‚¬ìš©ëœ ëª¨ë¸ì´ ì •ì˜ëœ í´ë˜ìŠ¤
|
â”œâ”€â”€ utils/                    - ìœ í‹¸ë¦¬í‹° ê´€ë ¨ í´ë”
â”‚   â”œâ”€â”€ __init__.py           - 
â”‚   â”œâ”€â”€ utils.py              - í™˜ê²½ ì„¤ì •ê³¼ ê°™ì€ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
â”‚   â”œâ”€â”€ train_utils.py        - ëª¨ë¸ í•™ìŠµì‹œ í•„ìš”í•œ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
|
â”œâ”€â”€ config.py                 - ëª¨ë¸ í•™ìŠµì‹œ ì„¤ì •í•˜ëŠ” íŒŒë¼ë¯¸í„°(argparse)
â”œâ”€â”€ FashionMNIST.py           - Fashion MNIST ë°ì´í„°ë¥¼ í™œìš©í•œ ë¨¸ì‹ ëŸ¬ë‹ ì˜ˆì œ
â”œâ”€â”€ linear_regression.py      - ìŠ¤ì¼ˆë ˆí†¤ ì˜ˆì œ
â”œâ”€â”€ predict.py                - 
â””â”€â”€ train.py                  - ì‹¤ì œ í•™ìŠµì´ ì´ë£¨ì–´ì§€ëŠ” main
```



## Sub2

### Req. 1. ì´ë¯¸ì§€ ë°ì´í„° ì „ì²˜ë¦¬

#### <u>Req. 1-1. ì´ë¯¸ì§€ íŒŒì¼ ë¡œë“œ</u>

> data > preprocess.py

   - ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ì‹¤ì œ ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ ë¦¬í„´í•˜ëŠ” í•¨ìˆ˜ë¥¼ êµ¬í˜„
   - ì´ë¯¸ì§€ ë³„ë¡œ í¬ê¸°ê°€ ë‹¤ë¥¸ ê²½ìš°ì—ëŠ” ëª¨ë‘ ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ ê°™ì€ ì‚¬ì´ì¦ˆë¡œ ë³€í˜•

```python
import os
import sys
import csv
import pickle
import gzip
from datetime import datetime
from config import config ğŸ“Œ
import numpy as np ğŸ“Œ
import matplotlib.pyplot as plt
import tensorflow as tf ğŸ“Œ
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing import sequence
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle

sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(__file__))))

# Sub2 Req. 1-1. batch_size ê°œìˆ˜ë§Œí¼ ì´ë¯¸ì§€ ë°ì´í„° ë¡œë”©
def loading_imgs(img_paths):
    img = tf.io.read_file(img_paths) # íŒŒì¼ì„ ì½ìŒ
    img = tf.image.decode_jpeg(img, channels=3) # ì´ë¯¸ì§€ íŒŒì¼ì„ ë””ì½”ë”©
    img = tf.image.resize(img, (config.image_size, config.image_size)) #ì´ë¯¸ì§€ í¬ê¸° ë³€í™˜
    img = tf.keras.applications.inception_v3.preprocess_input(img)

    return img, img_paths

# â“ ì•„ë˜ë¶€ë¶„ì€ ì–´ë–¤ ê¸°ëŠ¥ì„ í• ê¹Œìœ  ? ì–¸ì œ ì“°ì¼ê¹Œìœ  ? 
def map_func(img_name, cap):
    img_tensor = np.load(img_name.decode('utf-8') + '.npy')
    return img_tensor, cap
```

ğŸ“‘ img_paths, config.image_size



âœ `tf.io.read_file(ì´ë¯¸ì§€_íŒŒì¼ê²½ë¡œ)` : íŒŒì¼ì„ ì½ìŒ

âœ `tf.image.decode_jpeg(img_ì½ì–´ì˜¨íŒŒì¼, channels=3)` : íŒŒì¼ì„ ë””ì½”ë”©

âœ `tf.image.resize(img_ë””ì½”ë”©í•œíŒŒì¼, (config.image_size, config.image_size)_resizeí• í¬í‚¤)` : ì´ë¯¸ì§€ í¬ê¸° ë³€í™˜

âœ `tf.keras.applications.inception_v3.preprocess_input(img)`

- inception_v3.preprocess_input(img) :  ì´ë¯¸ì§€ë„· ë°ì´í„° ì„¸íŠ¸ì˜ í‰ê·  RGB ì±„ë„ì„ ëºŒ. ì´ë ‡ê²Œ í•˜ëŠ” ì´ìœ ëŠ” `inception_v3` ëª¨ë¸ì´ í•™ìŠµ í•  ë•Œì˜ ì „ì²˜ë¦¬ ê³¼ì •ì´ê¸° ë•Œë¬¸ => ì´ ëª¨ë¸ì˜ í•™ìŠµí•  ë•Œì™€ ê°™ì€ ì „ì²˜ë¦¬ë¥¼ í•´ì£¼ê¸° ìœ„í•¨

  import í•  ë•Œ `from tensorflow.keras.applications import inceptio_v3` ì´ë ‡ê²Œ í•˜ë©´ ë”ìš± ê°„ëµí•˜ê²Œ  ì‚¬ìš© ê°€ëŠ¥

- `inception_v3` ëª¨ë¸ : ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” ì´ë¯¸ì§€ ì¸ì‹ ëª¨ë¸ë¡œì„œ ImgeNet ë°ì´í„°ì„¸íŠ¸ì—ì„œ 78.1% ì´ìƒì˜ ì •í™•ì„±ì„ ì‹¤í˜„

âœ `np.load(img_name.decode('utf-8') + '.npy')` : npíŒŒì¼ì„ ë¡œë“œí•˜ëŠ”ë° ì´ë¯¸ì§€ ì´ë¦„ì„ ë””ì½”ë“œ í•˜ê³  .npyë¥¼ ë¶™ì„



#### <u>Req. 1-2. ì´ë¯¸ì§€ ì •ê·œí™”</u>

> data > preprocess.py

   - ì „ì²´ ì´ë¯¸ì§€ ë°ì´í„°ì˜ RGB ê°’ì˜ í‰ê· ê³¼ ë¶„ì‚°ì„ êµ¬í•´ ë°ì´í„°ë¥¼ ì •ê·œí™” í•˜ëŠ” í•¨ìˆ˜ë¥¼ êµ¬í˜„
   - í•´ë‹¹ í•¨ìˆ˜ë¥¼ ì´ë¯¸ì§€ 1-1 ê¸°ëŠ¥ê³¼ í•©ì³ ì´ë¯¸ì§€ íŒŒì¼ì„ ë¡œë“œí•  ë•Œ ì •ê·œí™” ì—¬ë¶€ê°€ ì „ë‹¬ë˜ê³  ì´ì— ë§ì¶° ì •ê·œí™” ëœ ë°ì´í„° ë˜ëŠ” ì •ê·œí™” ë˜ì§€ ì•Šì€ ë°ì´í„°ë¥¼ ì½ì–´ì˜¤ë„ë¡ í•¨

```python
import os
import sys
import csv
import pickle
import gzip
from datetime import datetime
from config import config
import numpy as np ğŸ“Œ
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing import sequence
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle

# Sub2 Req. 1-2. ì´ë¯¸ì§€ ì •ê·œí™”
def image_normalization(imgs):
    mean = np.mean(imgs, axis=(0, 1)) # axis: (x, y) ê¸°ì¤€ìœ¼ë¡œ í‰ê·  ê³„ì‚°
    std = np.var(imgs, axis=(0, 1)) # axis: (x, y) ê¸°ì¤€ìœ¼ë¡œ ë¶„ì‚° ê³„ì‚°

    imgs = (imgs-mean)/std

    return imgs
```

ğŸ“‘ imgs

âœ `np.mean(imgs, axis=(0, 1))`

- `numpy.mean(a, axis=None, dtype=None, out=None, keepdims=<no value>)`

  mean í•¨ìˆ˜ëŠ” ëª…ì‹œëœ ì¶•ì„ ë”°ë¼ì„œ ì‚°ìˆ í‰ê· ì„ ê³„ì‚°í•œë‹¤. ì´ í•¨ìˆ˜ëŠ” ë°°ì—´ ìš”ì†Œì˜ í‰ê· ê°’ì„ ë°˜í™˜í•¨. ë°˜í™˜ë˜ëŠ” í‰ê· ê°’ì€ ê¸°ë³¸ê°’ìœ¼ë¡œëŠ” ë°°ì—´ ë‚´ ëª¨ë“  ìš”ì†Œì— ëŒ€í•œ í‰ê· ê°’ì´ë©° í•„ìš”í•  ê²½ìš° ê° ì¶•ì— ëŒ€í•œ í‰ê· ê°’ì„ êµ¬í•  ìˆ˜ ìˆë‹¤.

  \* ê° ì…ë ¥ë³€ìˆ˜ì— ëŒ€í•œ ì„¤ëª…

  | ë³€ìˆ˜     | ì…ë ¥ìë£Œí˜•             | í•„ìˆ˜ì…ë ¥ì—¬ë¶€ | ì„¤ëª…                                                         |
  | -------- | ---------------------- | ------------ | ------------------------------------------------------------ |
  | a        | array_like             | í•„ìˆ˜         | - í‰ê· ê°’ì„ êµ¬í•˜ê¸° ìœ„í•´ ìˆ«ìí˜• ìë£Œí˜•ì„ ë‹´ê³  ìˆëŠ” ë°°ì—´ / - aê°€ ë°°ì—´ì´ ì•„ë‹ê²½ìš° arrayë¡œì˜ ì»¨ë²„ì „ì„ ì‹œë„í•¨ |
  | axis     | None, int, tuple(ints) | ì„ íƒ         | - í‰ê· ê°’ì„ êµ¬í•˜ê¸° ìœ„í•œ ì¶• í˜¹ì€ ì—¬ëŸ¬ê°œì˜ ì¶• / - ê¸°ë³¸ê°’ì€ Noneì´ë©°, ì´ ê²½ìš° ë°°ì—´ ì•ˆì— ëª¨ë“  ìš”ì†Œì˜ í‰ê· ê°’ì„ êµ¬í•œë‹¤. / - ì •ìˆ˜ë¡œ êµ¬ì„±ëœ íŠœí”Œì´ ì…ë ¥ë  ê²½ìš°, ì—¬ëŸ¬ ì¶•ì— ëŒ€í•œ í‰ê· ì„ ê³„ì‚°í•œë‹¤ |
  | dtype    | dyte-type              | ì„ íƒ         | - í‰ê· ì„ ê³„ì‚°í•˜ëŠ”ë° ì‚¬ìš©í•˜ëŠ” ìë£Œí˜• ì˜ë¯¸ / - ì •ìˆ˜ ë°°ì—´ì´ í‰ê· ì„ êµ¬í•˜ê¸° ìœ„í•´ ì…ë ¥ëœ ê²½ìš°, ê¸°ë³¸ ìë£Œí˜•ì€ float64 |
  | out      | nd array               | ì„ íƒ         | - ê²°ê³¼ë¥¼ ë°°ì¹˜í•  ëŒ€ì²´ ì¶œë ¥ ë°°ì—´ì„ ì…ë ¥ ë°›ìŒ / - ê¸°ë³¸ê°’ì€ None / - ë§Œì•½ ì…ë ¥ë˜ëŠ” ê²½ìš°, ì˜ˆìƒë˜ëŠ” ë°°ì—´ì˜ í˜•ìƒ(í¬ê¸°)ì´ ë³€ìˆ˜ì— ì…ë ¥ë˜ëŠ” ë°°ì—´ì˜ í˜•ìƒê³¼ ë™ì¼í•´ì•¼ í•¨ |
  | keepdims | bool                   | ì„ íƒ         | - ê¸°ë³¸ê°’ì€ ì•„ë¬´ê°’ë„ ì—†ëŠ” ìƒíƒœ / True : ê²°ê³¼ ê³„ì‚° í›„ ì œê±°ë˜ì–´ì§€ëŠ” ì¶•ë“¤ì„ ë‚¨ì•„ìˆê²Œ í•¨ |



âœ `np.var(imgs, axis=(0, 1))`

- `numpy.var(a, axis=None, dtype=None, out=None, keepdims=<no value>)`

  : ëª…ì‹œëœ ì¶•ì„ ë”°ë¼ì„œ ë¶„ì‚°ì„ ê³„ì‚°. ë°°ì—´ ìš”ì†Œì˜ ë¶„ì‚°ê°’ì„ ë°˜í™˜

  ë°˜í™˜ë˜ëŠ” ë¶„ì‚°ê°’ì€ ê¸°ë³¸ê°’ìœ¼ë¡œëŠ” ë°°ì—´ ë‚´ ëª¨ë“  ìš”ì†Œì— ëŒ€í•œ ë¶„ì‚°ê°’ì´ë©°, í•„ìš”í•  ê²½ìš° ê° ì¶•ì— ëŒ€í•œ ë¶„ì‚°ê°’ì„ êµ¬í•  ìˆ˜ ìˆìŒ

  \* ê° ì…ë ¥ë³€ìˆ˜ì— ëŒ€í•œ ì„¤ëª…

  | ë³€ìˆ˜     | ì…ë ¥ìë£Œí˜•             | í•„ìˆ˜ì…ë ¥ì—¬ë¶€ | ì„¤ëª…                                                         |
  | -------- | ---------------------- | ------------ | ------------------------------------------------------------ |
  | a        | array_like             | í•„ìˆ˜         | - ë¶„ì‚°ì„ êµ¬í•˜ê¸° ìœ„í•´ ìˆ«ìí˜• ìë£Œí˜•ì„ ë‹´ê³  ìˆëŠ” ë°°ì—´ / - aê°€ ë°°ì—´ì´ ì•„ë‹ê²½ìš° arrayë¡œì˜ ì»¨ë²„ì „ì„ ì‹œë„í•¨ |
  | axis     | None, int, tuple(ints) | ì„ íƒ         | - ë¶„ì‚°ì„ êµ¬í•˜ê¸° ìœ„í•œ ì¶• í˜¹ì€ ì—¬ëŸ¬ê°œì˜ ì¶• / - ê¸°ë³¸ê°’ì€ Noneì´ë©°, ì´ ê²½ìš° ë°°ì—´ ì•ˆì— ëª¨ë“  ìš”ì†Œì˜ ë¶„ì‚°ê°’ì„ êµ¬í•œë‹¤. / - ì •ìˆ˜ë¡œ êµ¬ì„±ëœ íŠœí”Œì´ ì…ë ¥ë  ê²½ìš°, ì—¬ëŸ¬ ì¶•ì— ëŒ€í•œ ë¶„ì‚°ì„ ê³„ì‚°í•œë‹¤ |
  | dtype    | dyte-type              | ì„ íƒ         | - ë¶„ì‚°ì„ ê³„ì‚°í•˜ëŠ”ë° ì‚¬ìš©í•˜ëŠ” ìë£Œí˜• ì˜ë¯¸ / - ì •ìˆ˜ ë°°ì—´ì´ í‰ê· ì„ êµ¬í•˜ê¸° ìœ„í•´ ì…ë ¥ëœ ê²½ìš°, ê¸°ë³¸ ìë£Œí˜•ì€ float64 |
  | out      | nd array               | ì„ íƒ         | - ê²°ê³¼ë¥¼ ë°°ì¹˜í•  ëŒ€ì²´ ì¶œë ¥ ë°°ì—´ì„ ì…ë ¥ ë°›ìŒ / - ê¸°ë³¸ê°’ì€ None / - ë§Œì•½ ì…ë ¥ë˜ëŠ” ê²½ìš°, ì˜ˆìƒë˜ëŠ” ë°°ì—´ì˜ í˜•ìƒ(í¬ê¸°)ì´ ë³€ìˆ˜ì— ì…ë ¥ë˜ëŠ” ë°°ì—´ì˜ í˜•ìƒê³¼ ë™ì¼í•´ì•¼ í•¨ |
  | keepdims | bool                   | ì„ íƒ         | - ê¸°ë³¸ê°’ì€ ì•„ë¬´ê°’ë„ ì—†ëŠ” ìƒíƒœ / True : ê²°ê³¼ ê³„ì‚° í›„ ì œê±°ë˜ì–´ì§€ëŠ” ì¶•ë“¤ì„ ë‚¨ì•„ìˆê²Œ í•¨ |



### Req. 2. í…ìŠ¤íŠ¸ ë°ì´í„° ì „ì²˜ë¦¬

#### <u>Req. 2-1. í…ìŠ¤íŠ¸ ë°ì´í„° í† í°í™”</u>

> data > preprocess.py

   - í…ìŠ¤íŠ¸ ìƒíƒœì¸ ë°ì´í„°ë¥¼ í…ì„œí”Œë¡œìš° Tokenizer ë¥¼ ì´ìš©í•´ ê° ë‹¨ì–´ì— í•´ë‹¹í•˜ëŠ” ê³ ìœ  ìˆ«ìë¡œ ë³€í™˜(ì •ìˆ˜ ì¸ì½”ë”©)

     ì˜ˆ) I am hungry => [4, 19, 290]

   - ìº¡ì…˜ì˜ ì‹œì‘ê³¼ ëì„ í‘œì‹œí•˜ê¸° ìœ„í•´ \<start>, \<end> í† í° ì¶”ê°€

   - í•™ìŠµì— ìš©ì´í•˜ë„ë¡ ëª¨ë“  ìº¡ì…˜ì˜ ê¸¸ì´ë¥¼ ë§ì¶°ì£¼ëŠ”ë° ì´ ë•Œ \<pad> í† í°ì„ ì‚¬ìš©

     ì˜ˆ) ë§Œì•½ ìº¡ì…˜ì˜ ìµœëŒ€ ê¸¸ì´ë¥¼ 30ìœ¼ë¡œ ì •í•˜ê³  \<pad> í† í°ì„ 2ë¡œ ì •í•œë‹¤ë©´ ê²°ê³¼

     I am hungry => \<start> I am hungry \<end> => [0, 4, 19, 290, 1, 2, 2, ..., 2]

     ì´ ë•Œ ì „ì²´ ê¸¸ì´ëŠ” 30, 2ì˜ ê°œìˆ˜ëŠ” 25ê°œ

   - í…ì„œí”Œë¡œìš°ì— êµ¬í˜„ëœ Tokenizerë¥¼ ì°¾ì•„ë³´ë©° êµ¬í˜„

``` python
import os
import sys
import csv
import pickle
import gzip
from datetime import datetime
from config import config
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer ğŸ“Œ
from tensorflow.keras.preprocessing import sequence ğŸ“Œ
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle

# Sub2 Req. 2-1 í…ìŠ¤íŠ¸ ë°ì´í„° í† í°í™”
def text_tokenizer(tr_captions, val_captions):
    captions = tr_captions + val_captions
    max_length = calc_max_length_caption(captions)

    tokenizer = Tokenizer(num_words=config.top_k, oov_token="<unk>", filters='!"#$%&()*+.,-/:;=?@[\]^_`{|}~ ')
    
    ## ë‹¨ì–´-ìƒ‰ì¸ ë° ìƒ‰ì¸-ë‹¨ì–´ ë§¤í•‘
    tokenizer.word_index['<pad>'] = 0
    tokenizer.index_word[1] = '<start>'
    tokenizer.word_index['<start>'] = 1
    tokenizer.index_word[2] = '<end>'
    tokenizer.word_index['<end>'] = 2
    tokenizer.fit_on_texts(captions)
	
    ### ëª¨ë“  ì‹œí€€ìŠ¤ë¥¼ ê°€ì¥ ê¸´ ì‹œí€€ìŠ¤ì™€ ë™ì¼í•œ ê¸¸ì´ë¡œ ì±„ì›€
    tr_tokens = tokenizer.texts_to_sequences(tr_captions)
    tr_tokens = sequence.pad_sequences(tr_tokens, maxlen=max_length, padding='post')

    val_tokens = tokenizer.texts_to_sequences(val_captions)
    val_tokens = sequence.pad_sequences(val_tokens, maxlen=max_length, padding='post')

    return tokenizer, tr_tokens, val_tokens

# ìº¡ì…˜ ìµœëŒ€ê¸¸ì´ ì •í•˜ê¸°
def calc_max_length_caption(captions):
    max = 0
    # ìº¡ì…˜ì„ í† í°í™” => ê³µë°±ë¶„í•  => ë°ì´í„°ì˜ ëª¨ë“  ê³ ìœ ë‹¨ì–´ì— ëŒ€í•œ ì–´íœ˜ë¥¼ ì–»ì„ ìˆ˜ ìˆìŒ
    for caption in captions:
        word = caption.split(" ")
        word = [n for n in word if n != "." and n != "," and n != " "]
        length = len(word)
        if max < length:
            max = length

    return max
```

ğŸ“‘ tr_captions, val_captions, calc_max_length_caption, config.top_k

âœ `tokenizer = Tokenizer(num_words=config.top_k, oov_token="<unk>", filters='!"#$%&()*+.,-/:;=?@[\]^_{|}~ ')`

- num_words : ì–´íœ˜ì˜ í¬ê¸° ì œí•œ (ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´)
- oov_token : ë‹¤ë¥¸ ëª¨ë“  ë‹¨ì–´ë¥¼ unk(ì•Œìˆ˜ì—†ìŒ) í† í°ìœ¼ë¡œ ë°”ê¿ˆ

âœ `tokenizer.fit_on_texts(captions)` : ë‹¨ì–´ ì¸ë±ìŠ¤ë¥¼ êµ¬ì¶• / ì…ë ¥ì— ë§ê²Œ ë‚´ë¶€ì˜ word_indexë¥¼ ë§Œë“œëŠ” í•¨ìˆ˜ (ê° ë‹¨ì–´ë§ˆë‹¤ ê³ ìœ ì˜ indexë¥¼ í• ë‹¹)

âœ `tokenizer.texts_to_sequences(tr_captions)` : ë¬¸ìì—´ì„ ì •ìˆ˜ ì¸ë±ìŠ¤ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ / ë¬¸ì¥ì„ indexì˜ ì‹œí€€ìŠ¤ë¡œ ë§Œë“¬

âœ `sequence.pad_sequences(val_tokens, maxlen=max_length, padding='post')`

- ë¬¸ì¥ì˜ ê¸¸ì´ë¥¼ ê°€ì¥ ê¸´ ì‹œí€€ìŠ¤ ê¸¸ì´ë¡œ ë§ì¶°ì¤Œ
- ëª¨ìë¼ëŠ” ë¬¸ì¥ì€ padding ê°’ìœ¼ë¡œ ì±„ì›Œì§



#### <u>Req. 2-2. Tokenizer ì €ì¥ ë° ë¶ˆëŸ¬ì˜¤ê¸°</u>

> data > preprocess.py

   - í† í¬ë‚˜ì´ì ¸ë¥¼ ì €ì¥í•˜ê³  ë¶ˆëŸ¬ ì˜¬ ìˆ˜ ìˆëŠ” í•¨ìˆ˜ë¥¼ êµ¬í˜„

     ì´ ë•Œ í† í¬ë‚˜ì´ì ¸ íŒŒì¼ì€ íŒŒì´ì¬ pickleë¡œ ì €ì¥

```python
import os
import sys
import csv
import pickle ğŸ“Œ
import gzip
from datetime import datetime
from config import config
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing import sequence
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle

# Sub2 Req. 2-2 í”¼í´ ì €ì¥ ë° ë¶ˆëŸ¬ì˜¤ê¸°
def save_load(tokenizer):
    path = ".\\datasets\\textTokenizers\\"
    
    ## tokensë¥¼ ì €ì¥í•  í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±
    if not os.path.isdir(path):
        os.mkdir(path)

    name = "Tokenizer.pickle"
    name = "".join(i for i in name if i not in "\/:*?<>|")
    name = path + name

    with open(name, 'wb') as handle:
        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)

    return
```

âœ íŒŒì´ì¬ pickle ëª¨ë“ˆ

: ì¼ë°˜ í…ìŠ¤íŠ¸ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•  ë•ŒëŠ” íŒŒì¼ ì…ì¶œë ¥ì„ ì´ìš©í•˜ëŠ”ë°, ë¦¬ìŠ¤íŠ¸ë‚˜ í´ë˜ìŠ¤ê°™ì€ "í…ìŠ¤íŠ¸ê°€ ì•„ë‹Œ ìë£Œí˜•"ì€ ì¼ë°˜ì ì¸ íŒŒì¼ ì…ì¶œë ¥ ë°©ë²•ìœ¼ë¡œëŠ” ë°ì´í„°ë¥¼ ì €ì¥í•˜ê±°ë‚˜ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ë‹¤.

íŒŒì´ì¬ì—ì„œëŠ” ì´ì™€ ê°™ì€ í…ìŠ¤íŠ¸ ì´ì™¸ì˜ ìë£Œí˜•ì„ íŒŒì¼ë¡œ ì €ì¥í•˜ê¸° ìœ„í•˜ì—¬ `pickle` ì´ë¼ëŠ” ëª¨ë“ˆì„ ì œê³µí•œë‹¤.

- pickle ëª¨ë“ˆì„ ì´ìš©í•˜ë©´ ì›í•˜ëŠ” ë°ì´í„° ìë£Œí˜•ì˜ ë³€ê²½ì—†ì´ íŒŒì¼ë¡œ ì €ì¥í•˜ì—¬ ê·¸ëŒ€ë¡œ ë¡œë“œí•  ìˆ˜ ìˆë‹¤.

  ì˜ˆ) `open('text.txt', 'w')` ë°©ì‹ìœ¼ë¡œ ë°ì´í„°ë¥¼ ì…ë ¥í•˜ë©´ string ìë£Œí˜•ìœ¼ë¡œ ì €ì¥ë¨

  - pickleë¡œ ë°ì´í„°ë¥¼ ì €ì¥í•˜ê±°ë‚˜ ë¶ˆëŸ¬ì˜¬ ë•ŒëŠ” íŒŒì¼ì„ ë°”ì´íŠ¸í˜•ì‹ìœ¼ë¡œ ì½ê±°ë‚˜ ì¨ì•¼í•œë‹¤ (wb, rb)

    wbë¡œ ë°ì´í„°ë¥¼ ì…ë ¥í•˜ëŠ” ê²½ìš°ëŠ” .bin í™•ì¥ìë¥¼ ì‚¬ìš©í•˜ëŠ”ê²Œ ì¢‹ë‹¤

  - ì…ë ¥ `pickle.dump(data, file)`

    ```python
    import pickle
    list = ['a', 'b', 'c']
    with open('list.txt', 'wb') as f:
        pickle.dump(list, f)
    ```

  - ë¡œë“œ `ë³€ìˆ˜ = pickle.load(file)`

    : í•œì¤„ì”© íŒŒì¼ì„ ì½ì–´ì˜¤ê³  ë”ì´ìƒ ë¡œë“œí•  ë°ì´í„°ê°€ ì—†ìœ¼ë©´ `EOFError` ë°œìƒ

    ```python
    with open('list.txt', 'rb') as f:
        data = pickle.load(f) # ë‹¨ í•œì¤„ì”© ì½ì–´ì˜´
    
    >>> data
    ['a', 'b', 'c']
    ```



### Req. 3. Dataset ìƒì„± í•¨ìˆ˜ êµ¬í˜„

#### <u>Req. 3-1. tf.data.Dataset ìƒì„±</u>

> data > preprocess.py

   - ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œì™€ ìº¡ì…˜ì„ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ **ì´ë¯¸ì§€ ë°ì´í„° ë° í† í°í™”ëœ ìº¡ì…˜ ìŒ**ì„ ë¦¬í„´í•˜ëŠ”  í•¨ìˆ˜ë¥¼ êµ¬í˜„
   - ì´ ë•Œ ë¦¬í„´ ê°’ì€ í…ì„œí”Œë¡œìš° ë°ì´í„° í˜•ì‹ tf.data.Dataset ì„ ë”°ë¥´ë„ë¡ í•¨

```python
import os
import sys
import csv
import pickle
import gzip
from datetime import datetime
from config import config
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf ğŸ“Œ
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing import sequence
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle

# Sub2 Req. 3-1. tf.data.Dataset ìƒì„±
def convert_to_dataset(imgs, tokenizer):

    dataset = tf.data.Dataset.from_tensor_slices((imgs, tokenizer))

    return dataset
```

`tf.data.Dataset.from_tensor_slices((imgs, tokenizer))`

âœ TensorFlowì˜ `Dataset` : ëª¨ë¸ì— ë°ì´í„°ë¥¼ ì œëŒ€ë¡œ ê³µê¸‰í•˜ê¸° ìœ„í•´ì„œ ì…ë ¥ íŒŒì´í”„ë¼ì¸ì„ ë§Œë“¤ì–´ì„œ GPUë¡œ ë“¤ì–´ì˜¬ ë°ì´í„°ë¥¼ ë©ˆì¶°ìˆê²Œ í•˜ì§€ ì•Šë„ë¡ í•˜ëŠ” ì‘ì—…ì„ ì‰½ê²Œ ì²˜ë¦¬í•´ì£¼ëŠ” API

âœ `from_tensor_slices((imgs, tokenizer))` : (ë°ì´í„°ì…‹ ìƒì„± ë©”ì„œë“œ) ë¦¬ìŠ¤íŠ¸, ë„˜íŒŒì´, í…ì„œí”Œë¡œ ìë£Œí˜•ì—ì„œ ë°ì´í„°ì…‹ì„ ë§Œë“¤ ìˆ˜ ìˆìŒ



#### <u>Req. 3-2. Image Data Augmentation</u>

> data > preprocess.py

   - ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ ë°›ì•„ ë§¤ë²ˆ ëœë¤í•˜ê²Œ ë‹¤ì–‘í•œ Augmentationì„ ì ìš©í•œ ë°ì´í„°ë¥¼ ë¦¬í„´í•˜ëŠ” í•¨ìˆ˜ë¥¼ êµ¬í˜„
   - ì´ ë•Œ cf.keras.preprocessing.image ëª¨ë“ˆì„ ì°¸ê³ í•´ êµ¬í˜„
   - ì¶”í›„ í…ì„œí”Œë¡œìš° ë°ì´í„°ì…‹ì„ ìƒì„±í•  ë•Œ Augmentationëœ ë°ì´í„°ê°€ ìº¡ì…˜ê³¼ ë°”ì¸ë”© ë˜ë„ë¡ í•¨

   ```python
import os
import sys
import csv
import pickle
import gzip
from datetime import datetime
from config import config ğŸ“Œ
import numpy as np
import matplotlib.pyplot as plt ğŸ“Œ
import tensorflow as tf ğŸ“Œ
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing import sequence
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle

# Sub2 Req. 3-2. Image Data Augmentation
def image_augmentation(dataset):
    augmented_dataset = (
        dataset
        .map(augment, num_parallel_calls=tf.data.experimental.AUTOTUNE)
        .prefetch(tf.data.experimental.AUTOTUNE)
    )

    return augmented_dataset


def augment(image, captions):
    
    image_size = config.image_size
    crop_size = int(image_size * 0.85)

    image = tf.image.convert_image_dtype(image, tf.float32)
    image = tf.image.random_flip_left_right(image)
    image = tf.image.random_crop(image, size=[crop_size, crop_size, 3])
    image = tf.image.resize(image, (image_size, image_size))
    image = tf.image.random_brightness(image, max_delta=0.5)

    return image, captions

# Image Data Augmentation visualize
def visualize(raw ,original, augmented):
    fig = plt.figure()
    plt.subplot(1,3,1)
    plt.title('Original image')
    plt.imshow(raw)
    plt.subplot(1,3,2)
    plt.title('Normalization image')
    plt.imshow(original)
    plt.subplot(1,3,3)
    plt.title('Augmented image')
    plt.imshow(augmented)
    plt.show()
   ```

ğŸ“‘ config.image_size

âœ Augmentation : ì›ë˜ ë°ì´í„°ë¥¼ ë¶€í’€ë ¤ì„œ ì„±ëŠ¥ì„ ë” ì¢‹ê²Œ ë§Œë“œëŠ” ê²ƒ

- Augmentationì„ í•˜ëŠ” ì¤‘ìš”í•œ ì´ìœ 

  1) preprocessingê³¼ augmentation ì„ í•˜ë©´, ê±°ì˜ ì„±ëŠ¥ì´ ì¢‹ì•„ì§

  2) ì›ë³¸ì— ì¶”ê°€ë˜ëŠ” ê°œë…ì´ë¼ì„œ ì„±ëŠ¥ì´ ë–¨ì–´ì§€ì§€ ì•ŠìŒ

  3) ì‰½ê³  íŒ¨í„´ì´ ì •í•´ì ¸ìˆìŒ

- ì˜ˆ

  1) ì¢Œìš°ë°˜ì „ 2) ì´ë¯¸ì§€ë¥¼ ì˜ë¼ì¤Œ 3) ë°ê¸°ì¡°ì ˆ ë“±

- `.map(augment, num_parallel_calls=tf.data.experimental.AUTOTUNE)
          .prefetch(tf.data.experimental.AUTOTUNE)`

  - augment : ë°ì´í„° ì¦ëŒ€

  - `num_parallel_calls=tf.data.experimental.AUTOTUNE`

    - num_parallel_calls : ë³‘ë ¬ì²˜ë¦¬ ìˆ˜ì¤€ ì§€ì •, ë³‘ë ¬ ë°ì´í„°ì…‹ì„ ë¶ˆëŸ¬ì˜¤ê³  íŒŒì¼ì„ ì—¬ëŠ”ë° ê¸°ë‹¤ë¦¬ëŠ” ì‹œê°„ì„ ë‹¨ì¶•í•  ìˆ˜ ìˆìŒ
    - tf.data.experimental.AUTOTUNE : ì–´ë–¤ ìˆ˜ì¤€ì˜ ë³‘ë ¬ì²˜ë¦¬ê°€ tf.data ëŸ°íƒ€ì„ì— ì‚¬ìš©ë˜ëŠ”ì§€ì— ëŒ€í•´ ê²°ì •

  - `prefetch(tf.data.experimental.AUTOTUNE)`

    - prefetch = ê°€ì ¸ì˜¤ê¸° : ì „ì²˜ë¦¬ì™€ í›ˆë ¨ ìŠ¤í…ì˜ ëª¨ë¸ ì‹¤í–‰ì„ ì˜¤ë²„ë©.

       [`tf.data.Dataset.prefetch`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset?hl=ko#prefetch)  : ë°ì´í„°ê°€ ì†Œë¹„ë˜ëŠ” ì‹œê°„ê³¼ ë°ì´í„°ê°€ ìƒì„±ë˜ëŠ” ì‹œê°„ ê°„ì˜ ì˜ì¡´ì„±ì„ ì¤„ì¼ ìˆ˜ ìˆìŒ. ê°€ì ¸ì˜¬ ìš”ì†Œì˜ ìˆ˜ëŠ” í•˜ë‚˜ì˜ í›ˆë ¨ ìŠ¤í…ì—ì„œ ì†Œë¹„í•œ ë°°ì¹˜ì˜ ìˆ˜ì™€ ê°™ê±°ë‚˜ ì»¤ì•¼í•¨. ì´ ê°’ì„ ìˆ˜ë™ìœ¼ë¡œ ì¡°ì •í•˜ê±°ë‚˜ [`tf.data.experimental.AUTOTUNE`](https://www.tensorflow.org/api_docs/python/tf/data/experimental?hl=ko#AUTOTUNE)ìœ¼ë¡œ ì„¤ì •í•˜ë©´ tf.data ëŸ°íƒ€ì„ì´ ì‹¤í–‰ ì‹œì— ë™ì ìœ¼ë¡œ ê°’ì„ ì¡°ì •í•˜ë„ë¡ ë§Œë“¦.

      ==> í”„ë¦¬í˜ì¹˜ ë³€í™˜ì€ "í”„ë¡œë“€ì„œ" ì‘ì—…ê³¼ "ì»¨ìŠˆë¨¸"ì˜ ì‘ì—…ê³¼ ì˜¤ë²„ë©ì´ ê°€ëŠ¥í•  ë•Œë§ˆë‹¤ ì´ì ì„ ì œê³µ

  

**âœ tf.keras.preprocessing.image**

: ì´ë¯¸ì§€ ë°ì´í„°ì— ëŒ€í•œ ì‹¤ì‹œê°„ ë°ì´í„° ì¦ê°€ë¥¼ ìœ„í•œ ë„êµ¬ì„¸íŠ¸



âœ **tf.image ëª¨ë“ˆ** 

: ì´ë¯¸ì§€ ì²˜ë¦¬ ë° ë””ì½”ë”©, ì¸ì½”ë”© ì‘ì—…ì„ ìœ„í•œ ë‹¤ì–‘í•œ ê¸°ëŠ¥ì„ í¬í•¨

- `tf.image.convert_image_dtype(image, tf.float32)` : ìƒ‰ ê³µê°„ ê°„ ë³€í™˜

  ```python
  tf.image.convert_image_dtype(
      image, dtype, saturate=False, name=None
  )
  ```

- `tf.image.random_flip_left_right(image)` : ì´ë¯¸ì§€ë¥¼ ê°€ë¡œë¡œ ë¬´ì‘ìœ„ë¡œ ë’¤ì§‘ìŒ (ì™¼ìª½ì—ì„œ ì˜¤ë¥¸ìª½ìœ¼ë¡œ)

  ```python
  tf.image.random_flip_left_right(
      image, seed=None
  )
  ```

- `tf.image.random_crop(image, size=[crop_size, crop_size, 3])` : ì£¼ì–´ì§„ í¬ê¸°ë¡œ í…ì„œë¥¼ ë¬´ì‘ìœ„ë¡œ ìë¦„

  ```python
  tf.image.random_crop(
      value, size, seed=None, name=None
  )
  # size = [crop_height, crop_width, 3]
  # RGBì˜ ê²½ìš° 3??
  ```

- `tf.image.resize(image, (image_size, image_size))` : ì´ë¯¸ì§€ í¬ê¸° ì¡°ì •

  ```python
  tf.image.resize(
      images, size, method=ResizeMethod.BILINEAR, preserve_aspect_ratio=False,
      antialias=False, name=None
  )
  ```

- `tf.image.random_brightness(image, max_delta=0.5)` : ì„ì˜ì˜ ìš”ì¸ìœ¼ë¡œ ì´ë¯¸ì§€ì˜ ë°ê¸°ë¥¼ ì¡°ì •

  ```python
  tf.image.random_brightness(
      image, max_delta, seed=None
  )
  ```



âœ ì‚¬ìš©ëœ Matplotlib í•´ì„

```python
import matplotlib.pyplot as plt
```

```python
plt.figure() # figure ìƒì„±
plt.imshow(image) # ì´ë¯¸ì§€ ì¶œë ¥
plt.colobar() # RGB ì»¬ëŸ¬ë°”
plt.grid(False) # ëˆˆê¸ˆì„  í‘œì‹œ ì—¬ë¶€
plt.show() # ì‹œê°í™”
```

```python
plt.figure(figsize=(10,10))
for i in range(25):
    plt.subplot(5,5,i+1) # ê°€ë¡œ 5ë“±ë¶„ ì„¸ë¡œ 5ë“±ë¶„ í›„ i+1 ë²ˆì§¸ ì¹¸ì— ì¶œë ¥
    plt.xticks([]) # xì¶• ëˆˆê¸ˆ ê°’ ì„¤ì •(ì˜ˆì‹œì™€ ê°™ì´ ë¹ˆ ë°°ì—´ë¡œ ì§€ì •í•˜ë©´ ëˆˆê¸ˆì´ ì œê±°ë¨)
    plt.yticks([]) # yì¶• ëˆˆê¸ˆ ê°’ ì„¤ì •
    plt.grid(False)
    plt.imshow(train_images[i], cmap=plt.cm.binary)
    plt.xlabel(class_names[train_labels[i]])
plt.show()
```

```python
def plot_value_array(i, predictions_array, true_label):
  predictions_array, true_label = predictions_array[i], true_label[i]
  plt.grid(False)
  plt.xticks([])
  plt.yticks([])
  thisplot = plt.bar(range(10), predictions_array, color="#777777") # bar: ë§‰ëŒ€ê·¸ë˜í”„
  plt.ylim([0, 1]) # yì¶• ë²”ìœ„ ì œí•œ
  predicted_label = np.argmax(predictions_array)

  thisplot[predicted_label].set_color('red')
  thisplot[true_label].set_color('blue')

plot_value_array(0, predictions_single, test_labels)
_ = plt.xticks(range(10), class_names, rotation=45)
plt.show()
```





### Req. 4. ì´ë¯¸ì§€ ëª¨ë¸(Encoder) êµ¬í˜„

#### <u>Req. 4-1. Pre-trained ëª¨ë¸ë¡œ íŠ¹ì„± ì¶”ì¶œ</u>

> encoder.py

   - ì´ë¯¸ì§€ ë°ì´í„°ì˜ íŠ¹ì„±ì„ ë½‘ì•„ë‚¼ ìˆ˜ ìˆëŠ” í•¨ìˆ˜ë¥¼ êµ¬í˜„

     ì´ ë•Œ GPUë¥¼ ìµœëŒ€í•œ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ batch sizeë¥¼ ì¡°ì •

     => ì´ í•¨ìˆ˜ì˜ ì‚¬ìš© ì‹œì ì— ë”°ë¼ ì´ë¯¸ì§€ íŒŒì¼ì˜ Augmentation ê°€ëŠ¥ì—¬ë¶€ë‚˜ Decoderì— ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” GPU ì˜ ë©”ëª¨ë¦¬ ê³µê°„ì´ ë‹¬ë¼ì§€ê²Œ ë˜ë¯€ë¡œ ì´ì— ëŒ€í•´ ê³ ë ¤í•´ê°€ë©° êµ¬í˜„

```python
import tensorflow as tf
from config import config


class CNN_Encoder(tf.keras.Model):
    def __init__(self, embedding_dim):
        super(CNN_Encoder, self).__init__()
        self.fc = tf.keras.layers.Dense(embedding_dim)

    def call(self, x):
        x = self.fc(x)
        return x


# Req. 4-1. Pre-trained ëª¨ë¸ë¡œ ì´ë¯¸ì§€ íŠ¹ì„± ì¶”ì¶œ
def create_cnn():
    image_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')

    new_input = image_model.input
    hidden_layer = image_model.layers[-1].output
    image_feature_extract_model = tf.keras.Model(new_input, hidden_layer)

    # save pre-trained model
    image_feature_extract_model.save(config.cnn_model_path)


def load_cnn():
    model = tf.keras.models.load_model(config.cnn_model_path)
    # model.summary()

    return model


def load_image(image, tokens):
    image = tf.keras.applications.inception_v3.preprocess_input(image)
    return image, tokens
```

ğŸ“‘ tf.keras.Model, embedding_dim, config.cnn_model_path

âœ tf.keras.layers.Dense(embedding_dim) : ê·œì¹™ì ìœ¼ë¡œ ì—°ê²°ëœ NN ë ˆì´ì–´, ê¸°ë³¸ ì‹ ê²½ë§ ë§Œë“¤ê¸°

```python
tf.keras.layers.Dense(
    units, activation=None, use_bias=True, kernel_initializer='glorot_uniform', 
    bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None,
    activity_regularizer=None, kernel_constraint=None, bias_constraint=None,
    **kwargs
)
```

âœ tf.keras.applications.InceptionV3(include_top=False, weights='imagenet') : Inception v3 ì•„í‚¤í…ì²˜ë¥¼ ì¸ìŠ¤í„´ìŠ¤í™”

- `include_top` : ë„¤íŠ¸ì›Œí¬ì˜ ë§ˆì§€ë§‰ ê³„ì¸µìœ¼ë¡œ ë§¨ ìœ„ì— ì™„ì „ ì—°ê²° ê³„ì¸µì„ í¬í•¨í• ì§€ ì—¬ë¶€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë¶€ìš¸. ê¸°ë³¸ê°’ì€ `True`
- `weights` : `None`(ë¬´ì‘ìœ„ ì´ˆê¸°í™”), `imagenet`(ImageNetì—ì„œ ì‚¬ì „ í›ˆë ¨) ë˜ëŠ”ë¡œë“œ í•  ê°€ì¤‘ì¹˜ íŒŒì¼ì˜ ê²½ë¡œ ì¤‘ í•˜ë‚˜ . ê¸°ë³¸ê°’ì€ `imagenet`

âœ tf.keras.Model(new_input, hidden_layer) : `Model` í›ˆë ¨ ë° ì¶”ë¡  ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ì—¬ ë ˆì´ì–´ë¥¼ ê°ì²´ë¡œ ê·¸ë£¹í™”

> https://www.tensorflow.org/api_docs/python/tf/keras/Model?hl=ko-KRhttp

âœ tf.keras.models.load_model(config.cnn_model_path) : `model.save()` ë¡œ ì €ì¥ëœ ëª¨ë¸ì„ ë¡œë“œ



----

## ì‚¬ìš©ëœ Python ì½”ë“œë“¤

### argparse - ëª…ë ¹í–‰ ì˜µì…˜, ì¸ìì™€ ë¶€ì† ëª…ë ¹ì„ ìœ„í•œ íŒŒì„œ

```python
import argparse

# 1. argparse.ArgumentParser()ë¥¼ ìƒì†ë°›ì•„ ì •ì˜
parser = argparse.ArgumentParser(description='Process some integers.')
# 2. add_argument
parser.add_argument('integers', metavar='N', type=int, nargs='+',
                    help='an integer for the accumulator')
parser.add_argument('--sum', dest='accumulate', action='store_const',
                    const=sum, default=max,
                    help='sum the integers (default: find the max)')

# 3. parser.parse_args()
args = parser.parse_args()
print(args.accumulate(args.integers))
```

ìœ„ì˜ íŒŒì´ì¬ ì½”ë“œê°€ `prog.py` ë¼ëŠ” íŒŒì¼ì— ì €ì¥ë˜ì—ˆë‹¤ê³  ê°€ì •í•  ë•Œ, ëª…ë ¹í–‰ì—ì„œ ì‹¤í–‰ë˜ê³  ìœ ìš©í•œ ë„ì›€ë§ ë©”ì‹œì§€ë¥¼ ì œê³µí•  ìˆ˜ ìˆë‹¤:

```bash
$ python prog.py -h
usage: prog.py [-h] [--sum] N [N ...]

Process some integers.

positional arguments:
 N           an integer for the accumulator

optional arguments:
 -h, --help  show this help message and exit
 --sum       sum the integers (default: find the max)
```

```bash
$ python prog.py 1 2 3 4
4

$ python prog.py 1 2 3 4 --sum
10
```



### open() - íŒŒì¼ ì—´ê¸°

ê¸°ë³¸ë°©ì‹

```python
f = open("foo.txt", 'w')
f.write("Life is too short, you need python")
f.close()
```

withë¬¸ê³¼ ê°™ì´ ì“°ê¸° - withë¸”ë¡ì„ ë²—ì–´ë‚˜ëŠ” ìˆœê°„ ì—´ë¦° ê°ì²´ fê°€ ìë™ìœ¼ë¡œ closeë˜ì–´ í¸ë¦¬í•˜ë‹¤.

```python
with open("foo.txt", "w") as f:
    f.write("Life is too short, you need python")
```

"w" : íŒŒì¼ ì“°ê¸°, ê°™ì€ ì´ë¦„ì˜ íŒŒì¼ì´ ìˆìœ¼ë©´ ë‚´ìš©ì„ ì§€ìš°ê³  ìƒˆë¡œ ì‘ì„±

"a" : ê¸°ì¡´ íŒŒì¼ì— ë‚´ìš© ì¶”ê°€

"r" : íŒŒì¼ ì½ê¸°



### csv.reader()

ì§€ì •ëœ csvfileì˜ ì¤„ì„ ì´í„°ë ˆì´íŠ¸ í•˜ëŠ” íŒë…ê¸°(reader) ê°ì²´ë¥¼ ë°˜í™˜

```python
f = open(caption_file_path, 'r')
caption_file = csv.reader(f)
```



### sklearn - shuffle()

ì…ë ¥ëœ 1ê°œ ì´ìƒì˜ ë°°ì—´ì„ ë™ì¼í•œ ìˆœì„œë¡œ ì„ì–´ì¤€ë‹¤. 2ê°œ ì´ìƒì˜ ë°°ì—´ì´ ì…ë ¥ë  ë•ŒëŠ” ì„ì„ í–‰ì˜ ê°œìˆ˜ê°€ ê°™ì•„ì•¼ í•¨ì— ìœ ì˜í•œë‹¤.

```python
from sklearn.utils import shuffle

img_path_list, caption_list = shuffle(img_path_list, caption_list)
```



### sklearn - train_test_split()

```python
from sklearn.model_selection import train_test_split

train_img_paths, val_img_paths, train_captions, val_captions = 
train_test_split(img_paths, captions, test_size=0.2, random_state=0)
# train_img_pathsì—ëŠ” img_pathsì—ì„œ (1-0.2)ë§Œí¼ì˜ ë°ì´í„°ê°€
# val_img_pathsì—ëŠ” ë‚˜ë¨¸ì§€(0.2=test_size) ë§Œí¼ì˜ ë°ì´í„°ê°€ ë‚˜ë‰˜ì–´ ë“¤ì–´ê°„ë‹¤
# captionsë„ ë™ì¼
```

