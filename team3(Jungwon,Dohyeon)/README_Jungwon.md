# Tensorflow 강의

### Tensorflow1-1 오리엔테이션

- 기계학습 분류

- 여기서 하는 건 회귀와 분류임.

  - 회귀는 숫자로된 결과를 예측하는 것이고 분류는 카테고리(범주형) 형태의 결과를 예측(음성/양성)

- 머신러닝 알고리즘: 분류와 회귀문제를 풀기 위해서 사용되는 방법(아래는 유명한 알고리즘)
  - Decision Tree
  - Random Forest
  - KNN
  - SVM
  - Neural Network
- Neural Network (=인공신경망=딥러닝)
  - 인공신경망, 딥러닝이라고도 불림.
  - 모두 사람의 뉴런을 모방해서 만든 알고리즘.
- 인공지능 > 기계학습 > 딥러닝
- 딥러닝을 구현하는데 사용되는 라이브러리
  - Tensorflow
  - PyTorch
  - Caffe2
  - theano

### Tensorflow1-3 지도학습의 빅피쳐

- 지도학습하기 위한 과정

  1. 과거의 데이터를 준비함.

     - 독립변수 == 원인 , 종속변수 == 결과

  2. 모델의 구조를 만듬. (변수의 개수에 따라서 구멍의 개수가 달라짐.)

  3. 데이터로 모델을 학습(FIT)함.

  4. 이를 통해서 컴퓨터가 판매량=온도*2 가 된다는 걸 알아냄.

  5. 모델을 이용함. 

  6. 원인이 15도인걸 입력하면 판매량이 30개라는 걸 예상함. 


### Tensorflow1-4 Google Colaboratory - 환경설정

- jupyter notebook - 데이터 분석시 많이 사용
- colab notebook - google drive내에서 jupyter notebook 처럼 사용할 수 있도록 google에서 만든 서비스 
- 사용 방법
  1. 드라이브 - 새로만들기-더보기-연결할 앱 더보기-colaboratory 설치
  2. 새로만들기-더보기-colaboratory를 통해서 파일 생성
  3. 생성된 회색부분을 셀이라고 함. 
     - 실행단축키: ctrl+enter는 해당 셀만 실행시킴. shift+enter는 해당 셀 실행 후 다음 셀 생성.

### Tensorflow1-5 표를 다루는 도구 '판다스'

- 변수 

  - 변수들 사이에 관계가 있을 경우

    - 독립변수: 변수 중 원인이 되는 변수

    - 종속변후: 변수 중 결과가 되는 변수

- 하나의 표안에 독립변수와 종속변수가 같이 있는 데 지도학습은 이걸 구분하는 것에서 시작함.

  - 프로그램을 이용해서 독립변수와 종속 변수를 분리시킴. 

    - 아래는 변수를 분리해서 `독립`과 `종속`이라는 변수에 담음.

  - 이전에 `pandas` import 해줘야함.   `import pandas as pd`


### Tensorflow 1 - 6. 표를 다루는 도구 '판다스' (실습)

- 이번 수업에서 작성한 전체 코드는 가장 밑에 있음.

- csv: `,`를 기준으로 컬럼을 구분한 데이터

- 데이터 준비하기

  1.  `pandas` import 하기 

  2.  csv 파일에서 데이터 읽어오기

     - 파일경로라는 변수에 csv 처장
     - `pandas` 라이브러리에서 `read_csv()` 메소드 사용해서 레모레이드라는 변수에 저장
     - csv 파일을 자동으로 여기에서 사용할 수 있게 변환해 담아줌?

     ```python
     ###########################
     # 라이브러리 사용
     import pandas as pd
      
     ###########################
     # 파일로부터 데이터 읽어오기
     파일경로 = 'https://raw.githubusercontent.com/blackdew/tensorflow1/master/csv/lemonade.csv'
     레모네이드 = pd.read_csv(파일경로)
      
     파일경로 = 'https://raw.githubusercontent.com/blackdew/tensorflow1/master/csv/boston.csv'
     보스턴 = pd.read_csv(파일경로)
      
     파일경로 = 'https://raw.githubusercontent.com/blackdew/tensorflow1/master/csv/iris.csv'
     아이리스 = pd.read_csv(파일경로)
     ```

  3. 변수에 내가 원하는 모양으로 잘 담겼는지 확인함.

     ```python
     # 데이터의 모양확인
     print(레모네이드.shape)
     print(보스턴.shape)
     print(아이리스.shape)
     ```

     - 아래와 같이 출력됌. (행, 열)을 의미함. 

  4. 독립변수, 종속변수를 분리하기위해서 칼럼이름을 확인해야함. 

     ```python
     
     ```
   # 데이터 칼럼이름 확인
     print(레모네이드.columns)
     print(보스턴.columns)
     print(아이리스.columns)
     ```
  
     - 아래와 같이 출력이 되는데 index라는 이름의 object 형태로 담겨 있음. 
  
  5. 이후 독립이라는 변수와 종속이라는 변수로 나눠줌.  그리고 잘 나눠졌는지 모양 확인.
  
     ```python
   # 독립변수와 종속변수 분리
     독립 = 레모네이드[['온도']]
   종속 = 레모네이드[['판매량']]
     print(독립.shape, 종속.shape)
     ```

     - 아래와 같이 출력되는데 (행, 열)을 의미하며 당연히 하나의 열이니까 행의 개수==데이터개수

  6. 보스턴과 아이리스도 나눠줌. 

     - 보스턴(일단은 medv만 종속으로 해줌.)

       ```python
     독립 = 보스턴[['crim', 'zn', 'indus', 'chas', 'nox', 
                   'rm', 'age', 'dis', 'rad', 'tax',
                 'ptratio', 'b', 'lstat']]
       종속 = 보스턴[['medv']]
     print(독립.shape, 종속.shape)
       ```

       - [[]] 처럼 두번 감싸주는 이유는 해당 컬럼에 대한 내용을 표 형태로 넣기 위해서인듯

       - 아래처럼 출력됌. 즉 행이 506개이고 열이 13개인 표가 완성됌.

     - 아이리스

       ```python
  독립 = 아이리스[['꽃잎길이', '꽃잎폭', '꽃받침길이', '꽃받침폭']]
       종속 = 아이리스[['품종']]
     print(독립.shape, 종속.shape)
       ```

       - 아래처럼 출력됌. 

     - `.head()` : 테이블에서 상위 5개의 데이터를 출력할때 사용하는 method

       ```python
       
       ```
    # 각각의 데이터 확인해보기
       print(레모네이드.head())
  print(보스턴.head())
       print(아이리스.head())
     ```
     
     - 아래처럼 출력됌.
     
     ```

-  전체 코드

  ```python
  ###########################
  # 라이브러리 사용
  import pandas as pd
   
  ###########################
  # 파일로부터 데이터 읽어오기
  파일경로 = 'https://raw.githubusercontent.com/blackdew/tensorflow1/master/csv/lemonade.csv'
  레모네이드 = pd.read_csv(파일경로)
   
  파일경로 = 'https://raw.githubusercontent.com/blackdew/tensorflow1/master/csv/boston.csv'
  보스턴 = pd.read_csv(파일경로)
   
  파일경로 = 'https://raw.githubusercontent.com/blackdew/tensorflow1/master/csv/iris.csv'
  아이리스 = pd.read_csv(파일경로)
   
  ###########################
  # 데이터의 모양확인
  print(레모네이드.shape)
  print(보스턴.shape)
  print(아이리스.shape)
   
  ###########################
  # 데이터 칼럼이름 확인
  print(레모네이드.columns)
  print(보스턴.columns)
  print(아이리스.columns)
   
   
  ###########################
  # 독립변수와 종속변수 분리
  독립 = 레모네이드[['온도']]
  종속 = 레모네이드[['판매량']]
  print(독립.shape, 종속.shape)
   
  독립 = 보스턴[['crim', 'zn', 'indus', 'chas', 'nox', 
              'rm', 'age', 'dis', 'rad', 'tax',
              'ptratio', 'b', 'lstat']]
  종속 = 보스턴[['medv']]
  print(독립.shape, 종속.shape)
   
  독립 = 아이리스[['꽃잎길이', '꽃잎폭', '꽃받침길이', '꽃받침폭']]
  종속 = 아이리스[['품종']]
  print(독립.shape, 종속.shape)
   
   
  ###########################
  # 각각의 데이터 확인해보기
  print(레모네이드.head())
  print(보스턴.head())
  print(아이리스.head())
  ```





### Tensorflow 1 - 7. 레모네이드 판매 예측

- 지도학습 흐름

  1. 과거의 데이터를 준비함.
  
   - tensorflow 1-6 부분임.
  
     ```python
       # 데이터를 준비합니다.
       파일경로 = 'https://raw.githubusercontent.com/blackdew/tensorflow1/master/csv/lemonade.csv'
       레모네이드 = pd.read_csv(파일경로)
       레모네이드.head()   # 데이터 구조를 확인하기 위한 코드
       # 종속변수, 독립변수를 분리
       독립 = 레모네이드[['온도']]
       종속 = 레모네이드[['판매량']]
       print(독립.shape, 종속.shape) # 잘 나눠졌는지 확인.
     ```
  
2. 모델의 구조를 만듬.
  
   - 코드는 아래와 같음. 여기서 중요한건 `shape=[숫자]` 와 `.Dense(숫자)(X)` 에서 `숫자`임.
   
     전자는 독립변수의 개수이고 후자는 종속변수의 개수임. 이걸 위해서 1-6 수업에서 shape을 확인한 것.
   
   - 유전자 알고리즘을 사용하는데 아래는 아주 간단한 뉴런 하나임. 모델을 학습하기 이전의 상태.
   
   - `model.compile(loss='mse')` 은 모델이 학습할 방법을 정해주는 부분
   
     ```python
     
     ```
    # 모델을 만듭니다.
     X = tf.keras.layers.Input(shape=[1])
       Y = tf.keras.layers.Dense(1)(X)
       model = tf.keras.models.Model(X, Y)
       model.compile(loss='mse') # 모델이 학습할 방법을 정해주는 부분
       ```
   
  3. 모델에 데이터를 넣어서 학습시킴.

   - 코드는 아래와 같음.  여기서 중요한 건 `epochs=숫자` 로 여기서 숫자가 의미하는 건 전체 데이터를 몇 번 반복해서 학습할 것인지를 정해줌.  여기서는 1000번을 학습하라는 의미
  
   - 이렇게 학습된 모델을 얻을 수 있음.
  
   - 근데 의문은 같은 데이터를 반복해서 학습하는 게 의미가 있는 건가? 컴퓨터가 까먹는 것도 아니고 아니면 이게 주어진 데이터 1000개를 학습하라는 의미인가. 공부하면서 수정해놓을 것.(20.09.10)
  
   - `verbose=0`을 주면 학습하는 과정을 출력하지 않음.  따라서 아래처럼 코드 두번 써준 이유는 10번 하니 loss 가 너무 크게 나옴. 따라서 1000 번 이상으로 많이 해줘야하는데 이 과정을 계속 출력할 수는 없음. 따라서 `verbose=0` 을 주고 학습이 끝났을 때 10번만 더 해봄. 그러면서 loss 값 확인.
  
     ```python
       # 모델을 학습시킵니다. 
     model.fit(독립, 종속, epochs=1000, verbose=0)
       model.fit(독립, 종속, epochs=10)
     ```

  4.  모델을 이용함. 즉, 예측해봄.

   - `.predict()` 메소드를 이용해서 새로운 값을 학습한 모델에 넣어 예측해봄.
  
     ```python 
       # 모델을 이용합니다. 
     print(model.predict(독립))
       print(model.predict([[15]]))
     ```
  
- 전체 코드

  ```python
  ###########################
  # 라이브러리 사용
  import tensorflow as tf
  import pandas as pd
   
  ###########################
  # 데이터를 준비합니다.
  파일경로 = 'https://raw.githubusercontent.com/blackdew/tensorflow1/master/csv/lemonade.csv'
  레모네이드 = pd.read_csv(파일경로)
  레모네이드.head()
  # 종속변수, 독립변수
  독립 = 레모네이드[['온도']]
  종속 = 레모네이드[['판매량']]
  print(독립.shape, 종속.shape)
   
  ###########################
  # 모델을 만듭니다.
  X = tf.keras.layers.Input(shape=[1])
  Y = tf.keras.layers.Dense(1)(X)
  model = tf.keras.models.Model(X, Y)
  model.compile(loss='mse')
   
  ###########################
  # 모델을 학습시킵니다. 
  model.fit(독립, 종속, epochs=1000, verbose=0)
  model.fit(독립, 종속, epochs=10)
   
  ###########################
  # 모델을 이용합니다. 
  print(model.predict(독립))
  print(model.predict([[15]]))
  ```

  

### Tensorflow 1 - 8. 손실의 의미

- 1-7에서 사용한 코드인`model.fit(독립, 종속, epochs=10)` 에 대한 학습은 아래와 같음.

  - 파란색 부분은 정해준 학습 횟수에서 몇번째인지를 나타냄

  - 주황색 부분은 학습에 걸린 시간

  - 초록색 부분은 학습이 얼마나 진행되었는지를 나타냄. 즉, 각 학습이 끝날때마다 그 시점의 모델이 얼마나 정답에 가깝게 맞추고 있는지를 나타는 지표.

- 학습 흐름

  1. 독립, 종속 변수을 모델에 넣어줌.

  2. 예측을 함. 

  3. 이 모델이 얼마나 좋은지 평가하기 위해서 실제 정답인 종속과 예측 데이터를 비교함.

  4. 예측과 결과의 차이를 제곱함. 그리고 그 값의 평균을 구함. 이 평균 값이 Loss 임.

  5. 예측과 결과의 차이가 적어질 수록 평균값은 0에 수렴하게 됌. 즉 학습이 잘 된 모델임.

  6. 위에서 학습하면서 Loss 가 나오는데 이 Loss 값이 내가 원하는 수준까지 떨어지게 하는게 학습의 목표.

- loss를 떨어뜨리기 위해서 반복해서 학습함.  25까지 떨어짐. 

- 계속 반복해서 학습시키고 0까지 떨어지게 됌. 




### Tensorflow 1 - 10. 보스턴집값예측

- `평균값`을 쓸때 때때로 평균을 제대로 보여주지 못함. 왜냐하면 높은 값이 너무 높을 경우 평균을 끌어 올리기 때문에 이런 경우 이때의 `평균의 대표성`을 무너뜨리는 값들을 `이상치`라고 함. 이런 경우에는 평균 대신 `중앙값`을 사용함. 

- 코드는 아래와 같음.

  ```python
  ###########################
  # 라이브러리 사용
  import tensorflow as tf
  import pandas as pd
   
  ###########################
  # 1.과거의 데이터를 준비합니다.
  파일경로 = 'https://raw.githubusercontent.com/blackdew/tensorflow1/master/csv/boston.csv'
  보스턴 = pd.read_csv(파일경로)
  print(보스턴.columns)
  보스턴.head()
   
  # 독립변수, 종속변수 분리 
  독립 = 보스턴[['crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis', 'rad', 'tax',
              'ptratio', 'b', 'lstat']]
  종속 = 보스턴[['medv']]
  print(독립.shape, 종속.shape)
   
  ###########################
  # 2. 모델의 구조를 만듭니다
  X = tf.keras.layers.Input(shape=[13])
  Y = tf.keras.layers.Dense(1)(X)
  model = tf.keras.models.Model(X, Y)
  model.compile(loss='mse')
   
  ###########################
  # 3.데이터로 모델을 학습(FIT)합니다.
  model.fit(독립, 종속, epochs=1000, verbose=0)
  model.fit(독립, 종속, epochs=10)
   
  ###########################
  # 4. 모델을 이용합니다
  print(model.predict(독립[5:10]))
  # 종속변수 확인
  print(종속[5:10])
   
  ###########################
  # 모델의 수식 확인
  print(model.get_weights())
  ```

- 모델 수식을 확인할 수 있음. 명령문은 `print(model.get_weights())` 임.

  - w,b 값을 수식으로 나타내면 아래와 같음. 두번째 array가 b 값임.

    

- 모델을 학습하는 것의 목표는 w값과 b값을 찾는 것임.

  - 이러한 w를 가중치(weight)라고 함. 
  - 여기서 b를 편향(bias)이라고 부름.

  

- 종속변수가 2개일 경우

  - 종속변수가 두개이므로 최종 수식(모델)이 두개가 필요함.

  

### Tensorflow 1-14. 아이리스 품종 분류 + 1-15 +1-16

- 아이리스 품종 데이터 

  - 독립변수: 꽃잎길이, 꽃잎폭, 꽃받침길이, 꽃받침폭
  - 종속변수: 품종

- 회귀, 분류 알고리즘 

  - 종속변수의 타입에 따라서 어떤 알고리즘을 사용할지가 정해짐.

  - 숫자로 나타내지는 `양적`인 경우 `회귀`(regression) 사용함.

  - T/F 처럼 `범주형`인 경우에는 `분류`(classification)알고리즘을 사용함.

    

- 분류 알고리즘 사용하여 모델 만들기

  1. 과거의 데이터를 준비함.

     - 회귀를 이용할때와 달라진 점

       - `아이리스 = pd.get_dummies(아이리스)` 추가됌.

       - 사진과 같은 수식으로 나타낼 수 있으나 숫자로 나타내야함. 

       - 품종을 모두 칼럼으로 만들어주고 해당 품종인 경우 1로 바꿔줌. 이런 작업을 `원핫인코딩(onehot-encoding)`이라고 함.

       - 원핫인코딩 작업을 `padas`의 `get_dummies()` 메소드를 사용하여 만들 수 있음.

       - 학습하면서 3가지의 모델을 만들어야함. 따라서 모델의 구조를 만들때 출력 Y가 3개임.

         

       ```python
       
       ```
# 라이브러리 사용
       import tensorflow as tf
import pandas as pd
        
###########################
       # 1.과거의 데이터를 준비합니다.
파일경로 = 'https://raw.githubusercontent.com/blackdew/tensorflow1/master/csv/iris.csv'
       아이리스 = pd.read_csv(파일경로)
       아이리스.head()
        
       # 원핫인코딩
       아이리스 = pd.get_dummies(아이리스)
        
       # 종속변수, 독립변수
       독립 = 아이리스[['꽃잎길이', '꽃잎폭', '꽃받침길이', '꽃받침폭']]
       종속 = 아이리스[['품종_setosa', '품종_versicolor', '품종_virginica']]
       print(독립.shape, 종속.shape)
       ```

  2. 모델의 구조를 만듭니다.
  
     - 독립변수의 수가 4개, 종속변수의 수가 3개
      
     - 회귀를 이용할때와 달라진 점
      
       - `activation='softmax'` , `loss='categorical_crossentropy'`이 변경

     - 분류예측 

       - Sigmoid와 Softmax가 있음.

       - softmax는 확률로 예측, 분류함(0~100%).  

         (비가 올 확률은 30%, 동전의 앞뒷면이 나올 확률 50%)

       - 여기서는 softmax를 사용함. (분류 모델의 경우 softmax로 감싸줌. 회귀는 없음.)

       - 예시 

         - 1번째 행은 setosa일 확률이 100%인것.

         - 2번째 행은 setosa일 확률이 70%, virginica일 확률이 30%임. 

         - 회귀모델 같은 경우에는 softmax처럼 감싸주는 게 없음. 이런 softmax같이 출력값을 조절하는 함수를 activation함수라고 함.

         - 문제의 유형에 맞게 loss를 지정해줘야함. 분류와 회귀 다름. 

           - 분류: model.compile(`loss='categorical_crossentropy'`)
  - 회귀: model.compile(`loss='mse'`)
         
       - `metrics='accuracy'`를 추가해주면 정확도를 보여줌. loss는 0에 가까울수록 accuracy는 1에 가까울수록 모델의 예측 정확도가 높음. 
      
     ```python
     
     ```
  # 2. 모델의 구조를 만듭니다
     X = tf.keras.layers.Input(shape=[4])
       Y = tf.keras.layers.Dense(3, activation='softmax')(X)
     model = tf.keras.models.Model(X, Y)
       model.compile(loss='categorical_crossentropy',
                     metrics='accuracy')
     ```
     
    나머지 과정은 동일
      
     ```python
  # 3.데이터로 모델을 학습(FIT)합니다.
     model.fit(독립, 종속, epochs=1000, verbose=0)
     model.fit(독립, 종속, epochs=10)
      
     ###########################
     # 4. 모델을 이용합니다
   # 맨 처음 데이터 5개
     print(model.predict(독립[:5]))
   print(종속[:5])
      
     # 맨 마지막 데이터 5개
     print(model.predict(독립[-5:]))
     print(종속[-5:])
      
     ###########################
     # weights & bias 출력
     print(model.get_weights())
     ```


​     

   







### Numpy

- numpy(Numerical Python)

  - 데이터 셋을 효과적으로 다루기 위한 라이브러리

  - 고성능의 수치 계산을 위해 만들어진 라이브러리

  - 효율적인 데이터분석이 가능하도록 N차원의 배열 객체를 지원 (ndarray) 

    > N차원의 배열 객체가 필요한 이유 : 데이터의 대부분은 숫자 배열로 볼 수 있음.
    >
    > 파이썬 리스트로도 계산 할 수 있지만 numpy는 list에 비해서 빠른 연산을 지원하고 메모리를 효율적으로 사용함. 

- **numpy library**에서 자주 사용하는 함수

  - np.array -> 배열생성
  - np.zeros -> 0이 들어있는 배열 생성
  - np.ones -> 1이 들어있는 배열 생성
  - np.empty -> 초기화가 없는 값으로 배열을 반환
  - np.arrange(n) -> 배열 버전의 range 함수

  ```python 
  # numpy를 활용한 예제
  import numpy as np
  
  # 파이썬에서 리스트 생성시
  list(range(10))
  
  #numpy에서 리스트 생성
  array1 = np.array([1, 2, 3, 4, 5])
  print(array1)
  
  array2 = np.ones((3, 5), dtype=float)
  print(array2)
  
  array3 = np.zeros(10)
  print(array3)
  
  // 출력
  [1 2 3 4 5]
  [[1. 1. 1. 1. 1.]
  [1. 1. 1. 1. 1.]
  [1. 1. 1. 1. 1.]]
  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
  ```

  > 두번째 처럼 요소 하나가 실수이면 나머지도 모두 실수로 출력됌.
  >
  > 세번째 처럼 2차원 배열 만듬.
  >
  > 마지막처럼 모두 실수로 출력되게 할 수 있음.

> 배열 데이터 타입 
>
> python list는 정수, bool, 실수, 문자열 모두 하나의 리스트에 들어갈 수 있지만 numpy는 단일 타입으로만 구성되어야 함. 
>
> 세번째 처럼 하면 arr 배열의 데이터 타입을 int형으로 변경해줌.



- 배열데이터 dtype

  > int32는 32비트 int64는 64비트(64비트가 당연히 더 많은 데이터 저장가능)
  >
  > int64는 i8로 표현되기도 함. 서로 같음.
  >
  > U는 유니 코드

- 배열 만들기

  > 첫번째는 10개의 데이터가 들어간 int 타입의 배열 만들어 달라 근데zeros 이므로 0으로 채움.
  >
  > 두번째는 ones이므로 1로 채우는 데 튜플 형태로 (3,5)로 되어 있는데 배열을 3*5 배열로 만들어 달라.
  >
  > 세번째는 arrange는 파이썬에서 range와 같음. 0부터 20까지 2만큼 뛰면서 배열 하나를 만들어 달라.
  >
  > 네번째는 linspace는 0부터 1까지를 5로 나눠 주고 배열로 만들어 달라는 의미.

- 난수로 채워진 배열 만들기

  > 첫번째 난수로 채워진 배열 만들기 위해서 random 함수 씀. (2,2)는 배열의 형태로 2*2로 만들어 달라.
  >
  > 두번째는 (평균, 표준편차, 배열의형태)로 평균이 0이고 표준편차가 1인 데이터를 2*2 형태로 만들어라.
  >
  > 세번째는 0부터 10까지의 랜덤한 정수를 2*2 형태로 만들어라. 

- 배열 만들기 1

> ndim은 배열의 차원 2차원 배열이니까 2가 출력됌. 
>
> size는 3*4 이니 12로 출력.

- 데이터에 접근하기 

  > 접근하고 변경 및 추가할때 파이썬 list와 같음.

- 데이터 찾고 잘라내기.

  > slicing 이것도 파이썬 list랑 비슷.

- 배열 모양바꾸기

  > reshape을 활요해서 바꿈. 여기서는 2*4 형태로 바꿈.

- 배열 이어 붙이고 나누기

  > ()안에 [x,y]로 써서 리스트 형태로 합침.

- 붙이는 방향 정하기

  > 2*2의 matrix를 만들고 axis가 0을 기준으로 아래로 matrix 두개를 합침.

  > 가로로 붙일때는 axis=1을 사용.

- 배열 나누기

  > split(나눌 배열, [기준인덱스], 방향)
  >
  > 여기서 위쪽 배열은  upper라는 만들어준 변수에 저장되고 아래쪽 배열은 lower에 저장.

>  가로 방향으로 나누기.

- Numpy 연산

  > 루프 사용하여 +5 더하기

> 데이터가 커지만 for문 사용시 느려짐.

- 행렬간 연산

- 브로드 캐스팅

  > shape이 다른 array끼리도 연산이 가능하게 해줌. (자동으로 해주는듯.)

>  이렇게 더해주면 자동으로 1,2,3 해당하는 거 더해줌.

> 이렇게 더해주면 세로로 되있는 것을 옆으로 당겨서 3X3으로 가로로 되어 있는 것을 아래로 당겨서 3X3으로 만들어 준다. 같은 크기로 만듬 그 후 연산. 

- 집계함수

  > 대용량 데이터에 직면했을때 첫단계는 궁금한 데이터에 대해서 요약 통계를 보는 것.
  >
  > 표준편차는 np.std(x)를 통해서 구할 수 있음.

> sum은 축을 통해서 계산할 수도 있음.
>
> 첫번째는 세로방향으로 더해라.
>
> 두번째는 가로방향으로 더해라.

- 마스킹연산

  > 해당 조건에 대해 T/F 반환하거나 T/F에 해당하는 값을 리스트로 반환





### Pandas

- pandas 

  - numpy기반 라이브러리로 구조화된 데이터를 효과적으로 처리하고 저장할 수 있는 파이썬 라이브러리
  - Array 계산에 특화된 numpy를 기반으로 만들어져서 다양한 기능들을 제공함. 
  - series data/ data frame

- serise

  - 특수한 dictionary
  - numpy array가 보강된 형태로 Data에 Index를 추가해서 가지고 있음.


> 인덱스로 접근 가능하며 지정해 줄 수 있음. 
>
> Series([1,2,3,4])를 통해서 1,2,3,4를 리스트에 넣어주고 index=['a','b','c','d'] 를 통해 각가의 데이터에 해당하는 인덱스를 지정해줌. 

> 딕셔너리로 series 만들기 딕셔너리의 키값은 index로 들어가고 value값은 data 값이 됌.

- DataFrame
  - 여러개의 series가 모여서 행과 열을 이룬 데이터

> 위에서 만든 population series 넣음.

> 파이썬에서 문자열은 기본적으로object로 봄.

> 1인당 GDP 만들어 보기

> 만든 데이터 프레임을 여러 형태의 파일로 저장가능
>
> CSV : Comma Sperated Values 으로 `,`로 구분되는 데이터

- Indexing과 Slicing

  - loc : 명시적인 인덱스를 참조하는 방식

    > country.loc['china'] 로 참조함. 오른쪽 표에서 해당 정보와 이름, 타입출력.

    > country.loc['japan:korea', :'population'] 은 japan에서 korea까지 데이터를 뽑을 건데 population 컬럼까지만 뽑아달라는 의미.(앞에 비워둔거 보니 처음부터를 의미하고 처음은 gdp)

  - iloc: 파이썬 스타일 정수 인덱스를 참조하는 방식. 

    > 여기서 인덱스는 앞에서부터 순서대로 정해짐. 그리고 당연히 [1:3] 일때 3은 포함하지 않음. 그래서 1부터 2까지임.

- DataFrame 새 데이터 추가/수정하기

  - 리스트로 추가하기, 딕셔너리로 추가하기

    > 첫번째 줄은 이름, 나이, 주소 라는 컬럼을 가진 DataFrame을 만들어 dataframe이라는 변수에 저장함.
    >
    > 두번째 줄은 리스트로 데이터를 추가하는 방식.
    >
    > 세번째 줄은 딕셔너리로 추가하는 방법.
    >
    > 네번째 줄은 데이터 수정.

- DataFrame 새 컬럼 추가

  > nan : Not a Number 으로 숫자가 아닌 데이터라는 의미, 값이 비어있는 데이터를 지칭함. (null과 같은 의미 인듯.)
  >
  > 여기서는 '전화번호'컬럼에 nan을 추가 해달라는 의미.

  > 두번째 줄을 보면 0번 index에 전화번호 컬럼에 '01012341234'를 넣어라

  > 세번째 줄을 보면 len을 쓰는데 몇개의  dataframe을 가지고 있나 보는 것. 

- 컬럼을 이용해서 데이터 선택하기

  - 컬럼 이름이 하나만 있다면 Serise

  - 리스트로 들어가 있다면 DataFrame, 특정컬럼만 뽑아서 DataFrame으로 만들어 주고 싶을때 쓰임. 

    >  저기 리스트에서 쓴 순서대로 해당 컬럼 데이터를 가져옴. 여기서는 이름, 주소, 나이로 쓰니까 그 순서대로 나옴. 

- 누락된 데이터 체크

  - 튜토리얼에서 보는 데이터와 달리 현실의 데이터는 누락되어 있는 형태가 많음.

  - `isnull()` 은 데이터가 비어있는지 확인하는 함수, 비어있으면 True 반환.

  - `notnull()`은 비어있지 않은 경우에 대해서 True 를 반환함.

    > 아래 데이터 프레임에 대해서 `isnull`, `notnull` 한 경우

> 누락된 데이터를 체크하는 걸 넘어서서 제거하기 위해서 `dropan()` 또는 채워넣기 위해서 `fillna()`를 씀.

> 첫번째 줄은 영희의 전화번호가 nan으로 비어 있으니까 해당 인덱스 데이터가 다 제거됌.

> 두번째 줄은 dataframe['전화번호'].fillna('전화번호 없음') 전화번호 컬럼에서 비어있으면 '전화번호 없음'으로 채워 주세요를 의미하는데 이렇게 하면  series 데이터로 나올텐데 이것을 다시 dataframe에 전화번호 컬럼에 넣어달라는 걸 의미함. 

- Series 연산

  - numpy array에서 사용했던 연산자들을 활용할 수 있음. 

    > 세번째 줄은 인덱스가 0인 것끼리 더해야 하는데 B에는 인덱스가  0인 데이터가 없음. 따라서 더해줄 수 없으니까 NaN을 반환. 인덱스가 3인 경우도 마찬가지. 둘다 해당 인덱스가 있어야 연산한 값을 반환함.

    > 네번째 줄 의미는 A에 B를 더하는 데 `fill_value` 을 값을 주면 해당 인덱스가 없을때 `fill_value`을 채워 연산을 함.

- DataFrame 연산

  > 첫번째 줄에서 randint(0,10,(2,2)) 의미가 0부터 10까지의 정수 중 랜덤하게 뽑아서 2*2 행렬을 만들어라. 그리고 컬럼값을 list로 A, B로 넣어라.

  > 더하기를 하는데 같은 이름을 가진 컬럼의 데이터끼리 더함.  A 데이터에서 인덱스가 0이고 컬럼 이름이 A인 2와 B 데이터에서 인덱스가 0이고 컬럼 이름이 A인 2와 더해서 4를 반환함. 

  > A+B를 했을때는 한쪽 데이터에 같은 인덱스와 컬럼이 없는 경우 NaN을 반환함.

- 집계함수

  - numpy array에서 사용했던 sum, mean등을 활용할 수 있다.

    > 세번째 줄은 A 라는 이름을 가진 컬럼. 즉, 해당 series를 모두 더해라. 

- 값으로 정렬하기

  > `sort_values()`을 이용해서 정렬할 것임. 아래는 정렬하기 전에 만든 dataframe

  > 함수에 컬럼을 인자로 받아서 해당 컬럼을 정렬함. 기본적으로 오름차순으로 정렬됌. (값을 기준으로 순서가 바뀌고 정렬됌. 인덱스 바뀐 것 확인)

  > 내림차순으로 정렬할때는 `ascending=False` 를 주면 됌.

  > 두개의 컬럼값을 리스트로 넣어서 정렬하면, 앞에것 먼저 정렬후에 두번째것을 정렬함. 예를 들어서 여기서 col2가 먼저 이므로 col2에 대해서 오름차순으로 정렬을 함. 그리고 이후 col1을 기준으로 정렬하는데 col2가 같은 경우에 대해서 정렬을 함. col2에서 A인 값이 두개임. 이 두개를 col1에서 1과 2를 정렬함. 

- 조건으로 검색하기

  - Numpy array와 마찬가지로 masking 연산이 가능함. 

    > numpy, pandas 라이브러리를 각각 np, pd 이름으로 가져옴.
    >
    > df 란 dataframe을 만들어 주고 A컬럼 중에 0.5 이하인 것을 출력하면 
    >
    > 오른쪽과 같은 series가 나옴. 

  - 조건에 맞는 DataFrame row를 추출 가능함.

    > 세번째 줄 `df["A"] < 0.5` 만 쓰면 결과가 1) 처럼 series 형태로 나오게 됌.  그리고 `df["B"] > 0.3` 으로 쓰면 2) 처럼 1)과 같은 series 형태로 출력됌. 따라서 df[] 로 감싸줘 Dataframe에 masking 현상을 적용해야 3)처럼 출력됌.  아니면 serise 형태로 출려됌.

    > 네번째 줄처럼 query를 써서 줄글로 연산하는 것도 가능함.  세번째 줄에서 and는 &으로 써야하는 듯. 세번째와 네번째 줄은 같은기능을 함.

  - 문자열이라면 다른 방식으로도 조건 검색이 가능.

    > 첫번째 줄은 Cat이라는 문자열을 포함하면 True 아니면 False를 반환함. 

    > 두번째 줄은 Cat이라는 문자열과 정확히 일치하면 True 아니면 False를 반환함.  `df["Animal"] == "Cat" `처럼 쓸 수도 있음.

    > 정규표현식을 통해서 좀 더 문자열을 강력하게 검색할 수 있다고 함. 이건 찾아보라고 함. 

     

​	

- 함수로 데이터 처리하기 

  - apply를 통해서 함수로 데이터를 다룰 수 있음.

    > 복잡한 연산을 하기 위해서 어떤 데이터를 받아 가공하고 가공된 값을 다시 데이터로 넣어주기 위해서 함수를 짜서 처리해야 되는 데 이때 apply를 씀. (다른 곳이랑 다르게 설명이 한 화면에 안 나와 밑에 자세히 씀.)

    > 첫번째 줄을 통해서  아래와 같은 dataframe이 만들어짐.


> 두번째 세번째 줄을 통해서 square 라는 함수를 만들고 제곱한 값을 반환함.

> 네번째 줄에서 apply를 사용하는데 이때 함수를 인자로 받음. 여기서는 square를 받음.  그러면 아래처럼 결과를 series 형태로 출력함. 

> 따라서 아까 dataframe에 추가하기위해서는  `df["square"]=df["Num"].apply(square)` 로 코드 작성해야함.  아래처럼 추가되어 출력됌.

> lambda를 이용할수도 있음. `x:x**2`의 의미는 x값을 받아 x**2을 실행하고 반환.
>
> `df["square"]=df.Num.apply(lambda x :x**2) `로 써줘 반환되는 값을 square 컬럼에 넣어줌. (`df["square"]=df["Num"].apply(square)` 와 같은 기능)



- apply를 통해서 함수로 데이터 다루는 다른 예시 1

  - 일정하지 않은 번호들을 숫자로 동일하게 변경하는 게 목표임.


> 아래와 같은 딕셔너리로 만들고 반복문을 돌면서 확인하고 replace를 통해서 key에 해당하는 값들을 value 값들로 바꿔  phone에 넣고 반환.

> 이후에 apply를 써서 함수를 인자로 받아 실행하고 preprocess_phone 컬럼에 집어넣음.

- 함수로 데이터 다루는 다른 예시 2

  - replace : apply 기능에서 데이터 값만을 대체하고 싶을때

  - 위에 처럼 데이터를 변경하는 함수를 짜고 apply에서 인자로 받아서 하는 게 아니라 데이터 값만들 바꾸고 싶을때 replace를 사용함. 

    > 변경될 값을 key로 변경하고 싶은 값을 value로 넣어서 딕셔너리 만들어줌.  첫번째 줄을 실행하면 1)과 2) 가운데 처럼 series 형태로 반환이 되고 이후 sex 컬럼에 다시 넣어주면 2)처럼 변경됌. 

> 위에 처럼 series로 받고 직접 넣어줘서 변경하고 싶은게 아니라 바로 dataframe에 적용시켜 2)처럼 바로 변경되길 원한다면 `inplace=True`로 주면 됌.  (inplace 혹시 replace에만 있는 속성인가? 알아보자 이전 강의에서 계속 series로 받고 이걸 다시 넣어주는 데 안그래도 될 상황이 더 많을 거 같음.)

- 그룹으로 묶기

  - 간단한 집계를 넘어서서 조건부로 집계하고 싶은 경우

    > 첫번째 두번째 줄 코드를 통해서 df에 key, data1, data2를 채움. 
    >
    > 세번째 줄 `groupby('key')` 에서 key값을 통해 묶어줌. 즉, 동일한 key값끼리 묶어줌. A는 A끼리 B는 B끼리, 이 코드를 통해서 일단 묶어서 가지고만 있음. 

    > 이후 네번째 줄을 통해서 어떤 연산을 실행함. 여기서는 sum인데  2) 이 그 결과임. key가 A인 것끼리 data1를 더하고 이를 data1 컬럼에 넣음.  

    > 다섯번째 줄 처럼 key, data1을 동시에 묶을 수 있음. 

- aggreate

  - groupby를 통해서 집계를 한번에 계산하는 방법

    > 첫번째 줄은 `groupby('key')` 를 통해서 key를 기준으로 묶고 `aggregate(['min', np.median, max])`를 통해서 묶인 값들을 서로 비교해서 최소값, 중간값, 최대값을 해당 컬럼을 만들고 값을 입력함.  (근데 왜 표현하는게 다르지 'min은 min' 이거인데 max는 왜 그냥 max일까? 그냥 문법 같기도 하고 )

    > 2) 보면 data1, data2로 크게 컬럼으로 나눠져 있고 그 밑에 min, median, max가 반복됌. 이런것을 멀티인덱스라고 부름. 

    > 두번째 줄을 실행하면 3)처럼 출력되는데 key를 기준으로 grouping 하고 data1 컬럼에는 최소값을 data2에는 sum을 하고 넣음. (이렇게 하면 컬럼마다 어떤 연산을 할지 정해줄 수 있으.있음.)

    > 첫번째 줄처럼 만들려면 리스트를 쓰고 두번째 줄 처럼 할때는 딕셔너리 써야하는 듯.

- filter

  - groupby를 통해서 그룹 속성을 기준으로 데이터 필터링

    > 세번째 줄 실행하면 key값으로 묶고 평균값을 반환함. 2) 표처럼 출력됌.

    > 네번째 줄을 실행하면 key값으로 묶고 filter 를 통해서 위해서 만든 함수를 인자로 받아 실해함. 함수는 x의 data2 컬럼의 평균값을 구하고 해당 값이 3을 넘으면 반환함. 
    >
    > 2) 표를 보면 key가 B, C인 값들이 3을 넘음 따라서 1)에서 B,C 모두를 반환함. 이때 인덱스 순으로 반환함. 0, 3 없는 건 A인게 빠져서 그럼. 

- apply

  - groupby를 통해서 묶인 데이터에 함수 적용

    > key를 기준으로 묶고 lambda를 이용해서 최대값에서 최소값을 빼줌. 

- get_group

  - groupby로 묶인 데이터에서 key값으로 데이터를 가져올 수 있다.

    > groupby로 묶은 데이터 중에서 필요한 값을 뽑아올때 씀. 세번째 줄 경우 시도를 기준으로 묶고 그 중에서 충남에 해당하는 데이터만 가져와라는 의미.
    >
    > 네번째 줄은 그 해당 데이터가 총 몇개인지 알아볼때 쓰임.

- MultiIndex

  - 인덱스를 계층적으로 만들 수 있다. 

    > index를 2차원 배열 형태로 집어넣음. ['A','A','B','B'] 가 순서가 더 높음으로 A와 B가 각각 하나로 묶을 것임.

  - 열 인덱스도 계층적으로 만들 수 있다. 

  - 다중 인덱스 컬럼의 경우 인덱싱은 계층적으로 한다. 인덱스 탐색의 경우에는 loc, iloc를 사용가능하다. 

    > 첫번째 줄을 시행하면 1)처럼 A 전체가 출력되고 두번째 줄을 실행하면 2)처럼 A의 1번 컬럼만 출력이 됌.  리스트 접근하는 거랑 비슷한듯 하다.  

    

- pivot_table

  - 데이터에서 필요한 자료만 뽑아서 새롭게 요약, 분석할 수 있는 기능.

  - 엑셀에서의 피봇 테이블과 같은 기능.

  - index는 행 인덱스로 들어갈 key값.

  - column에 열 인덱스로 라베링될 값.

  - value에 분석할 데이터.

    > 아래와 같은 테이블을 만들었다고 가정하고 밑에 설명 이어나감. 

  - 위의 데이터를 활용해서 성별과 좌석별 생존률 구하기

    > 컬럼 명으로 class가 오고 성별 데이터가 인덱스로 옴. 각의 데이터들, 첫번째줄은 성별이 female일때 class별 생존률의 평균을 나타냄. 

    > 이때 value값을 어떻게 채울지 정해주는게 aggfunc임. np.mean을 통해서 평균값을 채우겠다는 의미

  - 예시 2

    >index는 월별로 컬럼은 내역으로 정해지고 데이터 값들은 수입, 지출 내역으로 채워짐.



### Matplotlib

- matplotlib

  - numpy와 pandas로 데이터를 그래프나 차트로 시각화할때 사용하는 라이브러리

- 그래프 그려보기(numpy 이용해서 그래프 그리기)

  - pyplot 사용 (state mechine interface으로 자동으로 figure와 ax(?)를 생성해줌  )

    > plt로 줄여서 사용함. x와 y가 순서대로 묶여서 plot(x,y)에 들어가짐. (1,1), (2,2) .. 이런식으로 

    > 아래와 같이 title과 label 값을 줄 수 있음.

    

  - 다른 방식으로 그래프 그리기(객체기반 스타일)

    > 위에서 그리는 방식은 자동으로 figure와 ax가 생성되지만 이 방식은 직접 figure와 x를 생성을 해서  그래프를 그리게 됌. subplots에 인자를 아무것도 안넣으면 1개의 figure에 1개의 axes를 생성하게 됌. 

    > 근데 무슨 차이인지를 잘 모르겠음. 사용방법은 아래와 같음. 강사님은 이번 방식을 좀 더 선호한다고 함. 

    

    

  - matplotlib 구조

    > Figure :  그래프가 그려지는 전체 도화지라고 생각할 것.

    > Axes : Figure 안에 두개의 그래프가 있는데 이 그래프들을 axes (`엑시스` 또는 엑스)라고 부름.

    > line(line plot) : 점들이 이어져 선으로 표현되는 그래프.

    > Marker (scatter plot) : 선으로 표현되지 않고 점으로 찍혀 표현되는 그래프.

    > Grid : 격자무늬로 수정할 수 있음.

    > Major tick : 눈끔중에서 큰 눈끔을 지칭.

    > Minor tick : 눈끔중에서 큰 눈끔사이에 있는 작은 눈끔들을 지칭.

    > Legend : 각각의 그래프가 뭘 나타내는지 알려줌. (범례)

    

  - 그래프 저장하기

    > 여기서는 fig 하나에 ax 하나를 생성함. 저장할때 fig를 저장하는데 ` savefig`를 이용해서 저장. 이때 이름과 확장자 정해줌. 

    > dpi도 지정가능함. 300정도면 출력할때 문제없는 정도의 해상도임. 

    

  - 여러개 그래프 그리기

    > 첫번째줄은 np에 linspace() 함수를 이용함. 0부터 4*pi 까지 100개의 균등한 간격으로 나눈 데이터를 x라는 변수에 저장함. 

    > 두번째 줄은 plt의 subplots() 함수를 이용하는데 2와 1을 넣어서 인덱스가 0과 1인 그래프 두개를 그릴 것임. 

    > 세번째 줄과 네번째 줄은 plot을 이용하는 데 x 축에는 아까 저장한 x를 넣고, y 축에는 np.sin(x) 를 받아서 인덱스 0인 곳에 그려줌. 

    



- Line Plot

  - 코드 설명

    > subplots()  에 아무것도 입력하지 않으면  1개의 figure에 1개의 axes를 그림. 

    > 두번째 줄에서 arange()를 이용해서 0부터 14까지를 x에 저장함. \
    >
    > 아래 그래프 처럼 그래프 style을 지정해 줄 수 있음.

    

  - Line Style

    > 그래프 순서는 아래에서 위로 올라감. 
    >
    > 파란색 그래프는 solid로 끊김없이 쭉 이어지는 그래프. 

    > line style지정해 줄때 아래 사진처럼 기호로 줘도 되고 아니면 `- `대신 `solid `넣어줘도 됌. 

    

  - color

    > color 지정시에 rgb 또는 cmyk는 그냥 초성만 넣어줘도 적용됌.  

    > 0부터 1사이의 숫자를 넣어줄 수도 있는데 이는 gray 색상으로 그려지고 0은 흰색, 1은 검은색을 의미함. 

    > rgb에 대한 16진수 코드를 넣어줄 수도 있음. 

    

    

  - Marker

    > 많은 마커가 있지만 여기서는 5개만 소개함. 

    ​			 

  - 축 경계 조정

    > linspace를 이용해서 0부터 10사이를 1000으로 나눈 동일한 간격을 가진 숫자를 x에 집어넣음.

    > set_xlim 을 통해서 x 축이 어디에서 시작해서 어디에서 끝날지 정해줌. 
    >
    > set_ylim 을 통해서 y 축이 어디에서 시작해서 어디에서 끝날지 정해줌. 
    >
    > 이렇게 직접 정해주지 않으면 파이썬에서 기본으로 셋팅되어 있는 최적화된 것으로 보여줌.

    

  

  - 범례

    > 첫번째 줄을 통해서 1개의 figure에 1개의 ax를 그리고 두번째, 세번째 줄을 통해서 ax안에 두개의 line을 그림. 

    > label 속성을 지정해주어 범례 생성함. 

    > set_xlabel 과 set_ylabel을 통해서 x축과 y축의 이름 정해줌.

    > legend()를 통해서 범례 스타일을 정해줄 수 있음.  
    >
    > loc : 범례 위치지정(아래쪽일때는 lower, 중앙에 위치시킬때는 center),
    >
    > shadow,: 그림자 줄때  fancybox : 모서리 둥글게 만들때,  borderpad : 범례안쪽 padding 크기 조정할때 

    



- Scatter

  - 점으로 찍은 그래프.

    > plot으로 그리는 데 첫번째 인자 x축 값, 두번째 인자 y 축값, 여기까지는 line과 같음 . 세번째 인자를 "o"로 주면 scatter 그려짐. 

    > 이후 markersize, markerfacecolor, markeredgecolor를 줘서 style 변경 시켜줌. 

    

  - 여러개 찍을때 

    > scatter() 사용함. 먼저 x, y를 인자로 받았는데 첫번째 두번째 인자를 통해서 원의 중심이 어디인지를 정해줌. 이후 c를 정해주어 color를 정해고 s를 정해줘서 size를 정함. 
    >
    > alpha는 투명도를 정해주는 건데 0.3으로 줘서 겹쳐도 서로 보이도록 해줌.

    

- Bar plot

  - bar plot 그리기

    > subplots 을 통해서 figure 와 ax 생성. 다만 figsize를 통해서 figure 사이즈 정해줌. 
    >
    > bar() 를 이용해서 그려줌. 

    

  - bar plot 누적해서 그리기

    > 아래에서 쌓아올리는 느낌으로 그림.  bottom 을 이용해서 색깔별 시작을 정해줌. 
    >
    > 파랑색은 data에서 x를 이용한거고, 주황색은 y를 초록색은 z를 이용함. 
    >
    > set_xticks를 통해서 xticks를 정해주는데 아마도 인덱스같은 느낌인듯. 
    >
    > set_xticklabels를 통해서 xticks에 이름을 정해주는데 0,1,2로 나타야는데 A,B,C로 정해줘서 바꿔 표시됌. 

    > for 문 안쪽 내용 잘 이해안됌. (아무튼 중요한건 색깔별로 bottom을 정해주어 그려준다. 그리고 bottom은 이전 색깔의 끝임.)

    



- Histogram

  - Histogram(도수 분포)

    > hist()를 이용해서 그려줌.  bins를 통해서 얼마나 막대기를 몇개로 나눌건지 정해줌. 여기는 50개니깐 총 50개 막대로 나눔. 아래 그려준것은 5로 지정해줄때를 의미.


  

- Matplotlib with pandas(아래부터는 pandas를 이용해서 그래프 그리기)

  - plot 이용해서 그리기

    > 아래 dataframe을 그래프로 그리는 게 목적임.

    > plot()에서 첫번째인자 x축 정해주는데 order 컬럼(serise데이터)을 받음.  두번째 인자 y축 정해주는데 height를 받음. label을 height를 지정해줬는데 범례가 빠져 보이는 이유는 ax.legend() 이 코드가 빠졌기때문에 설정해줘도 표시가 안됌.

    > set_xlabel, set_ylabel을 통해서 x,y축 이름을 정해줌. 

    

  - Scatter 그래프 그리기.

    > 아래 데이터를 이용해서 공격과 방어에 대한 그래프로 그리는 게 목적.

    > 첫번째줄 코드는 위 dataframe에서 type1 또는 type2 가 fire이면 fire라는 dataframe에 저장
    >
    > `|` 이거나를 의미함. 두번째 코드도 마찬가지임.

    > scatter()을 이용해서 그려줌. 첫번째인자는 x축, 두번째 y축, 세번째 컬러지정, 네번째로 label을 지정해줌. marker로 모양 정해주고 s는 사이즈

    


